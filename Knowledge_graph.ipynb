{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Knowledge_graph.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZOU0_UQN_1t7"
      },
      "source": [
        "#Extract graphs for 2 time period"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_eVPdjfA4Nj"
      },
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "from collections import OrderedDict\n",
        "import numpy as np\n",
        "from itertools import permutations \n",
        "import tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3oxnTrRN5hVk"
      },
      "source": [
        "# cluster1 = ['http://dbpedia.org/resource/PHP']\n",
        "# cluster2 = ['http://dbpedia.org/resource/JavaScript']\n",
        "\n",
        "# cluster1 = ['http://dbpedia.org/resource/PHP', 'http://dbpedia.org/resource/ECMAScript']\n",
        "# cluster2 = ['http://dbpedia.org/resource/JavaScript', 'http://dbpedia.org/resource/Dynamic_HTML', 'http://dbpedia.org/resource/PHP']\n",
        "\n",
        "# cluster1 = ['http://dbpedia.org/resource/Fortran', 'http://dbpedia.org/resource/JavaScript', 'http://dbpedia.org/resource/PHP', 'http://dbpedia.org/resource/C++']\n",
        "# cluster2 = ['http://dbpedia.org/resource/ECMAScript', 'http://dbpedia.org/resource/JavaScript', 'http://dbpedia.org/resource/Dynamic_HTML', 'http://dbpedia.org/resource/VBScript']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ChKzjbzCmSDi"
      },
      "source": [
        "# getPathsBetweenTwoNodes Function returns dataframe of All possible paths between two Nodes\n",
        "def getPathsBetweenTwoNodes(A, B): \n",
        "  url = 'http://dbpedia.org/sparql/'\n",
        "  query = \"\"\"\n",
        "  SELECT *\n",
        "  WHERE\n",
        "  {\n",
        "    {\n",
        "    <\"\"\" + A + \"\"\">  ?r1 <\"\"\" + B + \"\"\"> .\n",
        "    }\n",
        "    UNION\n",
        "    {\n",
        "      <\"\"\" + A + \"\"\"> ?r1 ?node1 .\n",
        "      ?node1 ?r2 <\"\"\" + B + \"\"\"> .\n",
        "    }\n",
        "    UNION\n",
        "    {\n",
        "      <\"\"\" + A + \"\"\"> ?r1 ?node1 .\n",
        "      ?node1 ?r2 ?node2 .\n",
        "      ?node2 ?r3 <\"\"\" + B + \"\"\"> .\n",
        "    }\n",
        "    # Add additional UNION for each length of path you want up to your upper bound\n",
        "  }\n",
        "  \"\"\"\n",
        "  r = requests.get(url, params = {'format': 'json', 'query': query})\n",
        "  data = r.json()\n",
        "\n",
        "  subgraph = []\n",
        "  for item in data['results']['bindings']:\n",
        "      subgraph.append(OrderedDict({\n",
        "        'source_node': A, \n",
        "        'r1': item['r1']['value'],\n",
        "        'node1': item['node1']['value']\n",
        "            if 'node1' in item else \"\",\n",
        "        'r2': item['r2']['value']\n",
        "            if 'r2' in item else \"\",\n",
        "        'node2': item['node2']['value']\n",
        "            if 'node2' in item else \"\",\n",
        "        'r3': item['r3']['value']\n",
        "            if 'r3' in item else \"\",\n",
        "        'target_node': B\n",
        "        }))\n",
        "\n",
        "  df = pd.DataFrame(subgraph)\n",
        "  return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u03ZpRHYUwBH"
      },
      "source": [
        "def getFullGraph(cluster1, cluster2):\n",
        "  clusterNodesList = cluster1 + cluster2\n",
        "  clusterNodesList = list(dict.fromkeys(clusterNodesList))\n",
        "\n",
        "  perm = permutations(clusterNodesList, 2) \n",
        "  frames = []\n",
        "\n",
        "  for element in list(perm):\n",
        "    df = getPathsBetweenTwoNodes(element[0], element[1])\n",
        "    frames.append(df)\n",
        "\n",
        "  result = pd.concat(frames)\n",
        "  return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p734Ds0_LZQm"
      },
      "source": [
        "#Get the set of all unique Nodes and Edge sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13O3lrebtJ4u"
      },
      "source": [
        "def getUniqueNodesFromGraph(nodes_filter_graph):\n",
        "  # Get all Unique Nodes\n",
        "  unique_nodes_set = list(nodes_filter_graph.source_node.unique()) + list(nodes_filter_graph.node1.unique()) + list(nodes_filter_graph.node2.unique()) + list(nodes_filter_graph.target_node.unique())\n",
        "  unique_nodes_set = np.unique(unique_nodes_set)\n",
        "  # storing unique nodes in dataframe\n",
        "  unique_nodes_df = pd.DataFrame(unique_nodes_set.tolist(), columns=['Node'])\n",
        "  unique_nodes_df[\"id\"] = unique_nodes_df.index\n",
        "  unique_nodes_df.head()\n",
        "  return unique_nodes_df\n",
        "\n",
        "def getUniqueRelationsFromGraph(graphFull):\n",
        "  #Filtered table with only relations\n",
        "  edge_type_filter = graphFull[['r1', 'r2', 'r3']]\n",
        "  # Get set of edge types in the graph\n",
        "  edge_type_set = list(edge_type_filter.r1.unique()) + list(edge_type_filter.r2.unique()) + list(edge_type_filter.r3.unique())\n",
        "  edge_type_set = np.unique(edge_type_set)\n",
        "\n",
        "  # storing unique edges in dataframe\n",
        "  unique_edges_df = pd.DataFrame(edge_type_set.tolist(), columns=['Edge'])\n",
        "  unique_edges_df[\"id\"] = unique_edges_df.index\n",
        "  unique_edges_df.head(18)\n",
        "  return unique_edges_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-h9Bg3kgYh7G"
      },
      "source": [
        "#Map node and edge to respective id"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wc4AHsa8Ynmd"
      },
      "source": [
        "def mapNodeWithId(node):\n",
        "  if node != \"\":\n",
        "    df = unique_nodes_df.query('Node == \"' + node + '\"', inplace = False)\n",
        "    id = df[\"id\"].tolist()\n",
        "    return str(id[0])\n",
        "  else:\n",
        "    return \"\"\n",
        "\n",
        "def mapPredicateWithId(predicate):\n",
        "  if predicate != \"\":\n",
        "    df = unique_edges_df.query('Edge == \"' + predicate + '\"', inplace = False)\n",
        "    id = df[\"id\"].tolist()\n",
        "    return str(id[0])\n",
        "  else:\n",
        "    return \"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RsZMgEuEgNhS"
      },
      "source": [
        "**Get the Edge set**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXP3SH-Dn0Ss",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e7dba6c-16ec-48e6-fa40-a27d39542804"
      },
      "source": [
        "pd.set_option('max_rows', 99999)\n",
        "pd.set_option('max_colwidth', 400)\n",
        "pd.describe_option('max_colwidth')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "display.max_colwidth : int or None\n",
            "    The maximum width in characters of a column in the repr of\n",
            "    a pandas data structure. When the column overflows, a \"...\"\n",
            "    placeholder is embedded in the output. A 'None' value means unlimited.\n",
            "    [default: 50] [currently: 400]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ieQ2JMzSZsE7"
      },
      "source": [
        "def getAllRelationSets(graphFull):\n",
        "  # Subject -- Predicate -- object\n",
        "  relation_set = []\n",
        "  for index, row in graphFull.iterrows():\n",
        "    # for r1\n",
        "    if row[\"node1\"] == '':\n",
        "      relation_set.append([row[\"source_node\"], row[\"r1\"], row[\"target_node\"]])\n",
        "    else:\n",
        "      relation_set.append([row[\"source_node\"], row[\"r1\"], row[\"node1\"]])\n",
        "    # for r2\n",
        "    if row[\"node2\"] == '' and row[\"node1\"] != '':\n",
        "      relation_set.append([row[\"node1\"], row[\"r2\"], row[\"target_node\"]])\n",
        "    else:\n",
        "      relation_set.append([row[\"node1\"], row[\"r2\"], row[\"node2\"]])\n",
        "    # for r3\n",
        "    if row[\"node2\"] != '':\n",
        "      relation_set.append([row[\"node2\"], row[\"r3\"], row[\"target_node\"]])\n",
        "\n",
        "  relation_Dataframe = pd.DataFrame(relation_set, columns= ['start', 'relation', 'end'])\n",
        "  return relation_Dataframe"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmo1irBbEdt-"
      },
      "source": [
        "#Group Predicates, objects and object-predicate pair with their number of occurence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0tUQZpwOQKBk"
      },
      "source": [
        "**Unique Relation Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b7sUYpImwUnz"
      },
      "source": [
        "def getWeightCalcHelpers(relation_Dataframe):\n",
        "  unique_relations_count_series = relation_Dataframe.groupby(['start', 'relation', 'end']).size()\n",
        "  unique_relations_count_df = unique_relations_count_series.to_frame(name = 'size').reset_index()\n",
        "  unique_relation_Dataframe = unique_relations_count_df.copy()\n",
        "  unique_relation_Dataframe.drop(['size'], axis = 1)\n",
        "\n",
        "  predicate_count_series = relation_Dataframe.groupby(['relation']).size()\n",
        "  predicate_count_df = predicate_count_series.to_frame(name = 'size').reset_index()\n",
        "\n",
        "  object_count_series = relation_Dataframe.groupby(['end']).size()\n",
        "  object_count_df = object_count_series.to_frame(name = 'size').reset_index()\n",
        "\n",
        "  pred_obj_series = relation_Dataframe.groupby(['relation', 'end']).size()\n",
        "  pred_obj_count_df = pred_obj_series.to_frame(name = 'size').reset_index()\n",
        "\n",
        "  return unique_relation_Dataframe, predicate_count_df, object_count_df, pred_obj_count_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ae1XZbmp1GVc"
      },
      "source": [
        "#Semantic relation weighting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nuYbSNy51n9X"
      },
      "source": [
        "**Calculate IC for predicate and object**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zLiCRC7L1F95"
      },
      "source": [
        "def getICpredicate(predicate, predicate_count_df, all_pred_count):\n",
        "  given_pred_count = list(predicate_count_df.query('relation == \"' + predicate + '\"', inplace = False)[\"size\"])[0]\n",
        "  prob_pred = given_pred_count/all_pred_count\n",
        "  ic_pred = -np.log(prob_pred)\n",
        "  return ic_pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0xz93dLk5fwK"
      },
      "source": [
        "def getICobj(obj, object_count_df, all_obj_count):\n",
        "  given_obj_count = list(object_count_df.query('end == \"' + obj + '\"', inplace = False)[\"size\"])[0]\n",
        "  prob_obj = given_obj_count/all_obj_count\n",
        "  ic_obj = -np.log(prob_obj)\n",
        "  return ic_obj"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rSdgGshI6gSN"
      },
      "source": [
        "def getICobj_given_pred(obj, pred, predicate_count_df, pred_obj_count_df, total_relation):\n",
        "  given_pred_count = list(predicate_count_df.query('relation == \"' + pred + '\"', inplace = False)[\"size\"])[0]\n",
        "\n",
        "  prob_pred = given_pred_count/total_relation\n",
        "\n",
        "  obj_and_pred_count = list(pred_obj_count_df.query('end == \"'+ obj + '\" and relation == \"' + pred + '\"', inplace = False)[\"size\"])[0]\n",
        "\n",
        "  prob_obj_and_pred = obj_and_pred_count/total_relation\n",
        "\n",
        "  prob_obj_given_pred = prob_obj_and_pred/prob_pred\n",
        "  \n",
        "  ic_obj_and_pred = -np.log(prob_obj_given_pred)\n",
        "  return ic_obj_and_pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3zSNY43rAxZF"
      },
      "source": [
        "def getPMI(obj, pred, total_relation, predicate_count_df, pred_obj_count_df, object_count_df):\n",
        "  given_pred_count = list(predicate_count_df.query('relation == \"' + pred + '\"', inplace = False)[\"size\"])[0]\n",
        "\n",
        "  prob_pred = given_pred_count/total_relation\n",
        "\n",
        "  obj_and_pred_count = list(pred_obj_count_df.query('end == \"'+ obj + '\" and relation == \"' + pred + '\"', inplace = False)[\"size\"])[0]\n",
        "\n",
        "  prob_obj_and_pred = obj_and_pred_count/total_relation\n",
        "\n",
        "  prob_obj_given_pred = prob_obj_and_pred/prob_pred\n",
        "\n",
        "  given_obj_count = list(object_count_df.query('end == \"' + obj + '\"', inplace = False)[\"size\"])[0]\n",
        "  prob_obj = given_obj_count/total_relation\n",
        "\n",
        "  pmi = np.log(prob_obj_given_pred/prob_obj)\n",
        "  return pmi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0r1HkN5N9qCE"
      },
      "source": [
        "**Joint IC, combIC and ic+pmi**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3SZaCuoW9vom"
      },
      "source": [
        "def jointIC(pred, obj, predicate_count_df, pred_obj_count_df, all_pred_count, total_relation):\n",
        "  ic_pred = getICpredicate(pred, predicate_count_df, all_pred_count)\n",
        "  ic_obj_given_pred = getICobj_given_pred(obj, pred, predicate_count_df, pred_obj_count_df, total_relation)\n",
        "  weight = ic_pred + ic_obj_given_pred\n",
        "  return weight"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L1Vij3qY_Wrk"
      },
      "source": [
        "def combIC(pred, obj, predicate_count_df, object_count_df, all_pred_count, all_obj_count):\n",
        "  ic_pred = getICpredicate(pred, predicate_count_df, all_pred_count)\n",
        "  ic_obj = getICobj(obj, object_count_df, all_obj_count)\n",
        "  weight = ic_pred + ic_obj\n",
        "  return weight"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3E0EgTDa_4-c"
      },
      "source": [
        "def ic_and_pmi(pred, obj, predicate_count_df, pred_obj_count_df, object_count_df, all_pred_count, total_relation):\n",
        "  ic_pred = getICpredicate(pred, predicate_count_df, all_pred_count)\n",
        "  pmi_pred_obj = getPMI(obj, pred, total_relation, predicate_count_df, pred_obj_count_df, object_count_df)\n",
        "  weight = ic_pred + pmi_pred_obj\n",
        "  return weight"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3EuQ0t6gFsji"
      },
      "source": [
        "#Assign Weights on relation Dataframe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UfuTQEy8F6wG"
      },
      "source": [
        "**Joint IC, combIC, ic&pmi Scheme**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43TbGEuRxh_R"
      },
      "source": [
        "def assignWeightsToEdges(unique_relation_Dataframe, predicate_count_df, object_count_df, pred_obj_count_df, all_pred_count, all_obj_count, total_relation):\n",
        "  # Assign Weights to Edge set \n",
        "  edge_set_with_weight = []\n",
        "  for index, row in unique_relation_Dataframe.iterrows():\n",
        "    if row[\"start\"] != \"\" and row[\"relation\"] != \"\" and row[\"end\"] != \"\":\n",
        "      joint_ic_weight = jointIC(row[\"relation\"], row[\"end\"], predicate_count_df, pred_obj_count_df, all_pred_count, total_relation)\n",
        "      comb_ic_weight = combIC(row[\"relation\"], row[\"end\"], predicate_count_df, object_count_df, all_pred_count, all_obj_count)\n",
        "      ic_pmi_weight = ic_and_pmi(row[\"relation\"], row[\"end\"], predicate_count_df, pred_obj_count_df, object_count_df, all_pred_count, total_relation)\n",
        "      edge_set_with_weight.append([row[\"start\"], row[\"relation\"], row[\"end\"], joint_ic_weight, comb_ic_weight, ic_pmi_weight])\n",
        "  edge_set_weight_DF = pd.DataFrame(edge_set_with_weight, columns= ['Start', 'Relation', 'End', 'Joint_IC_Weight', 'Comb_IC_Weight', 'IC_PMI_weight'])\n",
        "  return edge_set_weight_DF"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GhXSfKBMmvqS"
      },
      "source": [
        "#Compute Cost of all the Edges\n",
        "---\n",
        "c(e) = wmax - w (e) ;\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8fQnx0Ykm1FA"
      },
      "source": [
        "def assignCostToEdges(edge_set_weight_DF):\n",
        "  edge_set_ic_Cost = []\n",
        "  max_joint_weight = edge_set_weight_DF['Joint_IC_Weight'].max()\n",
        "  max_comb_weight = edge_set_weight_DF['Comb_IC_Weight'].max()\n",
        "  max_ic_pmi_weight = edge_set_weight_DF['IC_PMI_weight'].max()\n",
        "\n",
        "  for index, row in edge_set_weight_DF.iterrows():\n",
        "    joint_cost = max_joint_weight - row[\"Joint_IC_Weight\"]\n",
        "    comb_cost = max_comb_weight - row[\"Comb_IC_Weight\"]\n",
        "    ic_pmi_cost = max_ic_pmi_weight - row[\"IC_PMI_weight\"]\n",
        "    edge_set_ic_Cost.append([row[\"Start\"], row[\"Relation\"], row[\"End\"], row[\"Joint_IC_Weight\"], joint_cost, row[\"Comb_IC_Weight\"], comb_cost, row[\"IC_PMI_weight\"], ic_pmi_cost])\n",
        "\n",
        "  edge_set_weight_cost_DF = pd.DataFrame(edge_set_ic_Cost, columns= ['Start', 'Relation', 'End', 'Joint_IC_Weight', 'Joint_Cost', 'Comb_IC_Weight', 'Comb_cost', 'IC_PMI_weight', 'IC_PMI_Cost'])\n",
        "  return edge_set_weight_cost_DF"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2EPYcH1gSHj9"
      },
      "source": [
        "#Assign total weight and total cost (with 3 schemes) of all paths in graphFull dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4KdKlv3XSNsb"
      },
      "source": [
        "def assignWeightAndCostToGraph(graphFull, edge_set_weight_cost_DF):\n",
        "  graphWithPathWeight = []\n",
        "  for index, row in graphFull.iterrows():\n",
        "    joint_weight = 0\n",
        "    comb_weight = 0\n",
        "    ic_pmi_weight = 0\n",
        "    if row[\"node1\"] == '':   #only one relation between source and target\n",
        "      df = edge_set_weight_cost_DF.query('Start == \"'+ row[\"source_node\"] + '\" and Relation == \"' + row[\"r1\"] + '\" and End == \"' + row[\"target_node\"] + '\"', inplace = False)\n",
        "      joint_weight = df[\"Joint_IC_Weight\"].values[0]\n",
        "      comb_weight = df[\"Comb_IC_Weight\"].values[0]\n",
        "      ic_pmi_weight = df[\"IC_PMI_weight\"].values[0]\n",
        "\n",
        "      joint_cost = df[\"Joint_Cost\"].values[0]\n",
        "      comb_cost = df[\"Comb_cost\"].values[0]\n",
        "      ic_pmi_cost = df[\"IC_PMI_Cost\"].values[0]\n",
        "      graphWithPathWeight.append([row[\"source_node\"], row[\"r1\"], \"\", \"\", \"\", \"\", row[\"target_node\"], joint_weight, comb_weight, ic_pmi_weight, joint_cost, comb_cost, ic_pmi_cost])\n",
        "    elif row[\"node1\"] != '' and row[\"node2\"] == '':  # one intermediate node between source and target\n",
        "      df1 = edge_set_weight_cost_DF.query('Start == \"'+ row[\"source_node\"] + '\" and Relation == \"' + row[\"r1\"] + '\" and End == \"' + row[\"node1\"] + '\"', inplace = False)\n",
        "      df2 = edge_set_weight_cost_DF.query('Start == \"'+ row[\"node1\"] + '\" and Relation == \"' + row[\"r2\"] + '\" and End == \"' + row[\"target_node\"] + '\"', inplace = False)\n",
        "      joint_weight = df1[\"Joint_IC_Weight\"].values[0] + df2[\"Joint_IC_Weight\"].values[0]\n",
        "      comb_weight = df1[\"Comb_IC_Weight\"].values[0] + df2[\"Comb_IC_Weight\"].values[0]\n",
        "      ic_pmi_weight = df1[\"IC_PMI_weight\"].values[0] + df2[\"IC_PMI_weight\"].values[0]\n",
        "\n",
        "      joint_cost = df1[\"Joint_Cost\"].values[0] + df2[\"Joint_Cost\"].values[0]\n",
        "      comb_cost = df1[\"Comb_cost\"].values[0] + df2[\"Comb_cost\"].values[0]\n",
        "      ic_pmi_cost = df1[\"IC_PMI_Cost\"].values[0] + df2[\"IC_PMI_Cost\"].values[0]\n",
        "      graphWithPathWeight.append([row[\"source_node\"], row[\"r1\"], row[\"node1\"], row[\"r2\"], \"\", \"\", row[\"target_node\"], joint_weight, comb_weight, ic_pmi_weight, joint_cost, comb_cost, ic_pmi_cost])\n",
        "    else:\n",
        "      df1 = edge_set_weight_cost_DF.query('Start == \"'+ row[\"source_node\"] + '\" and Relation == \"' + row[\"r1\"] + '\" and End == \"' + row[\"node1\"] + '\"', inplace = False)\n",
        "      df2 = edge_set_weight_cost_DF.query('Start == \"'+ row[\"node1\"] + '\" and Relation == \"' + row[\"r2\"] + '\" and End == \"' + row[\"node2\"] + '\"', inplace = False)\n",
        "      df3 = edge_set_weight_cost_DF.query('Start == \"'+ row[\"node2\"] + '\" and Relation == \"' + row[\"r3\"] + '\" and End == \"' + row[\"target_node\"] + '\"', inplace = False)\n",
        "      joint_weight = df1[\"Joint_IC_Weight\"].values[0] + df2[\"Joint_IC_Weight\"].values[0] + df3[\"Joint_IC_Weight\"].values[0]\n",
        "      comb_weight = df1[\"Comb_IC_Weight\"].values[0] + df2[\"Comb_IC_Weight\"].values[0] + df3[\"Comb_IC_Weight\"].values[0]\n",
        "      ic_pmi_weight = df1[\"IC_PMI_weight\"].values[0] + df2[\"IC_PMI_weight\"].values[0] + df3[\"IC_PMI_weight\"].values[0]\n",
        "\n",
        "      joint_cost = df1[\"Joint_Cost\"].values[0] + df2[\"Joint_Cost\"].values[0] +  + df3[\"Joint_Cost\"].values[0]\n",
        "      comb_cost = df1[\"Comb_cost\"].values[0] + df2[\"Comb_cost\"].values[0] + df3[\"Comb_cost\"].values[0]\n",
        "      ic_pmi_cost = df1[\"IC_PMI_Cost\"].values[0] + df2[\"IC_PMI_Cost\"].values[0] + df3[\"IC_PMI_Cost\"].values[0]\n",
        "      graphWithPathWeight.append([row[\"source_node\"], row[\"r1\"], row[\"node1\"], row[\"r2\"], row[\"node2\"], row[\"r3\"], row[\"target_node\"], joint_weight, comb_weight, ic_pmi_weight, joint_cost, comb_cost, ic_pmi_cost])\n",
        "\n",
        "  graphFullWith_ic_Weights_Cost = pd.DataFrame(graphWithPathWeight, columns= ['source_node', 'r1', 'node1', 'r2', 'node2', 'r3', 'target_node', 'Joint_Weight', 'Comb_Weight', 'IC_PMI_Weight', 'Joint_Cost', 'Comb_Cost', 'IC_PMI_Cost'])\n",
        "\n",
        "  return graphFullWith_ic_Weights_Cost"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pgqV7I0L2MQA"
      },
      "source": [
        "**Normalize all the cost**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v7qmqjVx2Qqd"
      },
      "source": [
        "def getGraphWithNormalizedCost(graphFullWith_ic_Weights_Cost):\n",
        "  max_joint_cost = graphFullWith_ic_Weights_Cost['Joint_Cost'].max()\n",
        "  max_comb_cost = graphFullWith_ic_Weights_Cost['Comb_Cost'].max()\n",
        "  max_ic_pmi_cost = graphFullWith_ic_Weights_Cost['IC_PMI_Cost'].max()\n",
        "\n",
        "\n",
        "  graphWithNormalizedCost = []\n",
        "  for index, row in graphFullWith_ic_Weights_Cost.iterrows():\n",
        "    normalized_joint = row[\"Joint_Cost\"] / max_joint_cost\n",
        "    normalized_comb = row[\"Comb_Cost\"] / max_comb_cost\n",
        "    normalized_ic_pmi = row[\"IC_PMI_Cost\"] / max_ic_pmi_cost\n",
        "    graphWithNormalizedCost.append([row[\"source_node\"], row[\"r1\"], row[\"node1\"], row[\"r2\"], row[\"node2\"], row[\"r3\"], row[\"target_node\"], row[\"Joint_Weight\"], normalized_joint, row[\"Comb_Weight\"], normalized_comb, row[\"IC_PMI_Weight\"], normalized_ic_pmi])\n",
        "\n",
        "  graphWithNormalizedCost_DF = pd.DataFrame(graphWithNormalizedCost, columns= ['source_node', 'r1', 'node1', 'r2', 'node2', 'r3', 'target_node', 'Joint_Weight', 'Joint_Cost', 'Comb_Weight', 'Comb_cost', 'IC_PMI_Weight', 'IC_PMI_Cost'])\n",
        "  return graphWithNormalizedCost_DF"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y30owAuse2xR"
      },
      "source": [
        "**Get Minimum cost between a pair of Nodes**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BQrxmZaze7U8"
      },
      "source": [
        "def getMinimumCost(df, source, target, weight_scheme):\n",
        "  filtered_df = df.query('source_node == \"'+ source + '\" and target_node == \"' + target + '\"', inplace = False)\n",
        "  \n",
        "  if filtered_df.empty:\n",
        "    cols = ['source_node', 'r1', 'node1', 'r2', 'node2', 'r3', 'target_node', 'Joint_Weight', 'Joint_Cost', 'Comb_Weight', 'Comb_cost', 'IC_PMI_Weight', 'IC_PMI_Cost']\n",
        "    temp_df = pd.concat([pd.DataFrame({k: [] for k in cols}), None, None])\n",
        "    return temp_df\n",
        "  \n",
        "  if weight_scheme == 'comb_ic':\n",
        "    return filtered_df[filtered_df.Comb_cost == filtered_df.Comb_cost.min()]\n",
        "  elif weight_scheme == 'ic_pmi':\n",
        "    return filtered_df[filtered_df.IC_PMI_Cost == filtered_df.IC_PMI_Cost.min()]\n",
        "  elif weight_scheme == 'joint_ic':\n",
        "    return filtered_df[filtered_df.Joint_Cost == filtered_df.Joint_Cost.min()]\n",
        "  else:\n",
        "    return filtered_df[filtered_df.Joint_Cost == filtered_df.Joint_Cost.min()]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wzqjwKPFzvDG"
      },
      "source": [
        "def getCheapestPath(df, source, target, weight_scheme):\n",
        "  df1 = getMinimumCost(df, source, target, weight_scheme)\n",
        "  df2 = getMinimumCost(df, target, source, weight_scheme)\n",
        "\n",
        "  if weight_scheme == 'comb_ic':\n",
        "    cost_name = 'Comb_cost'\n",
        "  elif weight_scheme == 'ic_pmi':\n",
        "    cost_name = 'IC_PMI_Cost'\n",
        "  elif weight_scheme == 'joint_ic':\n",
        "    cost_name = 'Joint_Cost'\n",
        "  else:\n",
        "    cost_name = 'Joint_Cost'\n",
        "\n",
        "  if not df1.empty:\n",
        "    c1 = df1[cost_name].values[0]\n",
        "  else:\n",
        "    c1 = 1\n",
        "\n",
        "  if not df2.empty:\n",
        "    c2 = df2[cost_name].values[0]\n",
        "  else:\n",
        "    c2 = 1\n",
        "\n",
        "  if c1 < c2:\n",
        "    return c1\n",
        "  else:\n",
        "    return c2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BgSVwVohxsAp"
      },
      "source": [
        "#Mapping Skills to Dbpedia links"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T2OvEi9R-pce"
      },
      "source": [
        "def annotate_with_Dbpedia_spotlight(text, confidence):\n",
        "  # text preprocessing\n",
        "  text = text.replace(\"_\", \" \").replace(\"-\", \" \")\n",
        "  URL = \"https://api.dbpedia-spotlight.org/en/annotate?text=\" + text + \"&confidence=\" + str(confidence) + \"\"\n",
        "  HEADERS = {'Accept': 'application/json'}\n",
        "  response = requests.get(URL, headers=HEADERS)\n",
        "  if response.status_code != 200:\n",
        "    return 0\n",
        "  \n",
        "  json_obj = response.json()\n",
        "  if \"Resources\" in json_obj:\n",
        "    return json_obj[\"Resources\"][0]['@URI']\n",
        "  else:\n",
        "    return 0\n",
        "\n",
        "# annotate_with_Dbpedia_spotlight('Machine learning', 0.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJ_fAhFg8_Yj"
      },
      "source": [
        "skill_df = pd.read_csv(\"/content/drive/My Drive/Colab Notebooks/Knowledge graph files/skill_name_list.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_7ZL5fHpVovm"
      },
      "source": [
        "skill_df.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AFxJJjlA-Hef"
      },
      "source": [
        "**Map Skill name to Link**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vT9bJQF1xy--"
      },
      "source": [
        "def map_skills(skills_to_be_mapped):\n",
        "# skills_to_be_mapped = ['mysql','oracle','postgresql','database']\n",
        "  cluster = []\n",
        "  for skill in skills_to_be_mapped:\n",
        "    link = annotate_with_Dbpedia_spotlight(skill, 0.5)\n",
        "    if link == 0:\n",
        "      confidence = 0.4\n",
        "      while confidence >= 0:\n",
        "        link = annotate_with_Dbpedia_spotlight(skill, confidence)\n",
        "        confidence = confidence - 0.1\n",
        "    if link == 0:\n",
        "      link_list = list(skill_df.query('name == \"'+ skill.lower() + '\"', inplace = False)[\"link\"])\n",
        "      if link_list:\n",
        "        link = link_list[0]\n",
        "      else:\n",
        "        link = 0\n",
        "    cluster.append(link)\n",
        "    print(link)\n",
        "  return cluster"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qpzxXQkk4iEw"
      },
      "source": [
        "# Build Graph with Nodes\n",
        "\n",
        "- And Assign weights and Cost to connecting Edges"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bwpXX3Dc43ws"
      },
      "source": [
        "def buildGraphWithCost(cluster1, cluster2):\n",
        "  print('Runing Sparql')\n",
        "  graphFull = getFullGraph(cluster1, cluster2)\n",
        "  # Filtered table with only nodes\n",
        "  nodes_filter_graph = graphFull[['source_node', 'node1', 'node2', 'target_node']]\n",
        "\n",
        "  unique_nodes_df = getUniqueNodesFromGraph(nodes_filter_graph)\n",
        "  unique_edges_df = getUniqueRelationsFromGraph(graphFull)\n",
        "\n",
        "  relation_Dataframe = getAllRelationSets(graphFull)\n",
        "\n",
        "  all_pred_count = relation_Dataframe.query('relation != \"\"', inplace = False).shape[0]\n",
        "  all_obj_count = relation_Dataframe.query('end != \"\"', inplace = False).shape[0]\n",
        "  total_relation = relation_Dataframe.query('relation != \"\"', inplace = False).shape[0]\n",
        "\n",
        "  unique_relation_Dataframe, predicate_count_df, object_count_df, pred_obj_count_df = getWeightCalcHelpers(relation_Dataframe)\n",
        "\n",
        "  # Assign Weights to Edge Sets\n",
        "  print('Assigning Weights')\n",
        "  edge_set_weight_DF = assignWeightsToEdges(unique_relation_Dataframe, predicate_count_df, object_count_df, pred_obj_count_df, all_pred_count, all_obj_count, total_relation)\n",
        "\n",
        "  # Compute Cost for Edge sets\n",
        "  edge_set_weight_cost_DF = assignCostToEdges(edge_set_weight_DF)\n",
        "\n",
        "  # Compute total weight and cost for all connecting paths between Nodes in the graph\n",
        "  print('Assigning Cost to Graph')\n",
        "  graphFullWith_ic_Weights_Cost = assignWeightAndCostToGraph(graphFull, edge_set_weight_cost_DF)\n",
        "\n",
        "  # Normalized Cost\n",
        "  graphWithNormalizedCost_DF = getGraphWithNormalizedCost(graphFullWith_ic_Weights_Cost)\n",
        "\n",
        "  return graphWithNormalizedCost_DF"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oJMNxLWk8R0M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5aa2471-8778-40b8-f21c-78bf2466b100"
      },
      "source": [
        "# cluster1 = ['.net','perl','nosql', 'groovy','.net-framework', 'selenium','javascript','sql','ansible','relational-database','git']\n",
        "# cluster2 = ['.net','continuous-integration','perl','nosql','groovy','postgresql','.net-framework','c++','selenium','oracle','mysql','scripting-language','javascript','sql','ansible','relational-database','database','git']\n",
        "\n",
        "# if len(cluster1) <= len(cluster2):\n",
        "#   smaller_cluster_len = len(cluster1)\n",
        "# else:\n",
        "#   smaller_cluster_len = len(cluster2)\n",
        "# # count common_elements\n",
        "# num_common_elements = (len(list(set(cluster1).intersection(cluster2)))/smaller_cluster_len) * 100\n",
        "\n",
        "# # remove the common nodes from two clusters\n",
        "# cluster1_new = list(set(cluster1) - set(cluster2))\n",
        "# cluster2_new = list(set(cluster2) - set(cluster1))\n",
        "# num_common_elements\n",
        "# # normalized_cost_df = buildGraphWithCost(cluster1, cluster2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "100.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZIytZ1lQ10Xa"
      },
      "source": [
        "# Prepare Edit Distance Matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fGwz8NsUCeuL"
      },
      "source": [
        "\n",
        "Matrix:\n",
        "\n",
        "```\n",
        "#             Ecma  Javascript HTML VB\n",
        "# Fortran    [ 0     x         y     z ]\n",
        "# Javascript [ a     b         c     z ]\n",
        "# PHP        [ a     b         c     z ]\n",
        "# C++        [ a     b         c     z ]\n",
        "```\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tHNaQW5SIIbh"
      },
      "source": [
        "**IC+PMI edit Distance Matrix**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LFEKzT9fbbWO"
      },
      "source": [
        "from scipy.optimize import linear_sum_assignment"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2QWfzlmH4oTB"
      },
      "source": [
        "**Prepare Matrix**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ehPey8kn4mqA"
      },
      "source": [
        "def prepare_edit_cost_matrix(weight_scheme, graphWithNormalizedCost_DF):\n",
        "  list_outer = []\n",
        "  for item in cluster1:\n",
        "    list_inner = []\n",
        "    for element in cluster2:\n",
        "      if item != element:\n",
        "        total_cost = getCheapestPath(graphWithNormalizedCost_DF, element, item, weight_scheme)\n",
        "      else:\n",
        "        total_cost = 0\n",
        "      list_inner.append(total_cost)\n",
        "    list_outer.append(list_inner)\n",
        "  final_matrix = np.array(list_outer, np.float64)\n",
        "  return final_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WTKHjRiB3Uc6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229
        },
        "outputId": "68812243-0f76-4029-a63f-2e73aef81a1f"
      },
      "source": [
        "cluster1 = ['http://dbpedia.org/resource/DevOps', 'http://dbpedia.org/resource/Security', 'http://dbpedia.org/page/Scrum_(software_development)',\n",
        "            'http://dbpedia.org/resource/Microservices', 'http://dbpedia.org/resource/Data_science']\n",
        "cluster2 = ['http://dbpedia.org/resource/Robotics', 'http://dbpedia.org/resource/DevOps', 'http://dbpedia.org/resource/Linux',\n",
        "            'http://dbpedia.org/resource/Microservices']\n",
        "# normalized_cost_df = buildGraphWithCost(cluster1, cluster2)\n",
        "\n",
        "# # computing costs as per 3 different schemes\n",
        "# ic_pmi_matrix = prepare_edit_cost_matrix('ic_pmi', normalized_cost_df)\n",
        "# row_ind, col_ind = linear_sum_assignment(ic_pmi_matrix)\n",
        "# ic_pmi_similarity_score = ic_pmi_matrix[row_ind, col_ind].sum()\n",
        "\n",
        "# joint_ic_matrix = prepare_edit_cost_matrix('joint_ic', normalized_cost_df)\n",
        "# row_ind, col_ind = linear_sum_assignment(joint_ic_matrix)\n",
        "# joint_ic_similarity_score = joint_ic_matrix[row_ind, col_ind].sum()\n",
        "\n",
        "# comb_ic_matrix = prepare_edit_cost_matrix('comb_ic', normalized_cost_df)\n",
        "# row_ind, col_ind = linear_sum_assignment(comb_ic_matrix)\n",
        "# comb_ic_similarity_score = comb_ic_matrix[row_ind, col_ind].sum()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-72-c1c4c3d2195d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# ic_pmi_matrix = prepare_edit_cost_matrix('ic_pmi', normalized_cost_df)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# row_ind, col_ind = linear_sum_assignment(ic_pmi_matrix)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mic_pmi_similarity_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mic_pmi_matrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol_ind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow_ind\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mic_pmi_similarity_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# joint_ic_matrix = prepare_edit_cost_matrix('joint_ic', normalized_cost_df)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: index 4 is out of bounds for axis 1 with size 4"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YxR7LsmPmgdo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c58079eb-3f29-49ee-8b64-98a9290b0a2a"
      },
      "source": [
        "print(ic_pmi_matrix)\n",
        "print(ic_pmi_similarity_score)\n",
        "print(\"==============\")\n",
        "print(joint_ic_matrix)\n",
        "print(joint_ic_similarity_score)\n",
        "print(\"==============\")\n",
        "print(comb_ic_matrix)\n",
        "print(comb_ic_similarity_score)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.76217169 0.         0.55189819 0.3261407 ]\n",
            " [0.46872839 0.6523435  0.4632961  0.67804354]\n",
            " [1.         1.         1.         1.        ]\n",
            " [0.65185475 0.3261407  0.53154725 0.        ]\n",
            " [0.32616079 0.84601947 0.65318381 0.97752862]]\n",
            "0.7894568909479087\n",
            "==============\n",
            "[[0.4179354  0.         0.17179201 0.18384327]\n",
            " [0.2207421  0.24131767 0.16309381 0.36684558]\n",
            " [1.         1.         1.         1.        ]\n",
            " [0.43918076 0.18384327 0.21843866 0.        ]\n",
            " [0.11733579 0.36667588 0.24726281 0.36599191]]\n",
            "0.2804296004108966\n",
            "==============\n",
            "[[0.68835032 0.         0.56225123 0.29842561]\n",
            " [0.46052879 0.57643795 0.48086796 0.66327371]\n",
            " [1.         1.         1.         1.        ]\n",
            " [0.60524661 0.29842561 0.52176652 0.        ]\n",
            " [0.34254474 0.7090817  0.58752702 0.73799421]]\n",
            "0.8234127002206915\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xOJXxsJ96su_"
      },
      "source": [
        "comb_ic_similarity_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W8f0zyu96kbw"
      },
      "source": [
        "print(ic_pmi_similarity_score)\n",
        "print(joint_ic_similarity_score)\n",
        "print(comb_ic_similarity_score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y1d6mMwORscO"
      },
      "source": [
        "# Compare two time period Graphs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mp-30eYnoku2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "900c600e-9fe9-4efa-87f3-1d2f1d39b2e1"
      },
      "source": [
        "# list(skill_df.query('name == \"compiler\"', inplace = False)[\"link\"])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ib1gcJPsoZam",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ec1eefe-9424-4503-8b74-869eaccb4d1c"
      },
      "source": [
        "cluster = map_skills(['version-control','softwre-engineering','continuous-integration','hybris','agile','c++','scripting-language'])\n",
        "print(cluster)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "http://dbpedia.org/resource/Version_control\n",
            "0\n",
            "http://dbpedia.org/resource/Continuous_integration\n",
            "http://dbpedia.org/resource/Hubris\n",
            "http://dbpedia.org/resource/Agile_software_development\n",
            "http://dbpedia.org/resource/C++\n",
            "http://dbpedia.org/resource/Scripting_language\n",
            "['http://dbpedia.org/resource/Version_control', 0, 'http://dbpedia.org/resource/Continuous_integration', 'http://dbpedia.org/resource/Hubris', 'http://dbpedia.org/resource/Agile_software_development', 'http://dbpedia.org/resource/C++', 'http://dbpedia.org/resource/Scripting_language']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2HwcrcNR4QS"
      },
      "source": [
        "period_a = [\n",
        "\t['devops','security','scrum','microservices','data-science'],\n",
        "\t['mysql','oracle','postgresql','database'],\n",
        "\t['cad','data-conversion'],\n",
        "\t['sql','ansible','.net','groovy','perl','.net-framework'],\n",
        "\t['tcl','project-management'],\n",
        "\t['statistics','compiler','monitoring','mongodb'],\n",
        "\t['responsive-web-design','design','multithreading','virtualization','sharepoint'],\n",
        "\t['data-management','data-warehouse','business-intelligence','data-modeling','big-data','architect','computer-science'],\n",
        "\t['data-analysis','elasticsearch','machine-learning','data-mining'],\n",
        "\t['javascript','selenium'],\n",
        "\t['embedded-linux','linux'],\n",
        "\t['software-architecture','matlab'],\n",
        "\t['html','user-interface','user-experience'],\n",
        "\t['nosql','relational-database','git'],\n",
        "\t['version-control','software-engineering','continuous-integration','hybris','agile','c++','scripting-language']\n",
        "]\n",
        "\n",
        "period_b = [\n",
        "\t['sqlite','continuous-integration','scripting-language'],\n",
        "\t['autosar','software-design'],\n",
        "\t['.net-framework','.net','sql','groovy','perl'],\n",
        "\t['robotics','devops','linux','microservices'],\n",
        "\t['oracle','matlab','json','xml','gradle','database','image-processing'],\n",
        "\t['mongodb','simulation','hardware','scada','design'],\n",
        "\t['extreme-programming','scrum'],\n",
        "\t['lucene','architect','data-science','big-data'],\n",
        "\t['artificial-intelligence','product-management','javascript','data-warehouse','jira','html','data-mining'],\n",
        "\t['data-analysis','business-intelligence','software-architecture','cloud'],\n",
        "\t['linked-data','relational-database','nosql','project-management'],\n",
        "\t['monitoring','version-control','git','ansible','elasticsearch'],\n",
        "\t['html5','amazon-web-services'],\n",
        "\t['sharepoint','refactoring','software-engineering','ios'],\n",
        "\t['php','c++','unit-testing','mysql','angularjs','symfony'],\n",
        "\t['statistics','computer-science','data-conversion','compiler','user-experience','user-interface'],\n",
        "\t['security','machine-learning','data-management','opengl','r']\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7gDx2OO0iFc5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6cd9f75-a82c-4a70-cd17-3e9fed0af7a6"
      },
      "source": [
        "for cluster_a in period_a:\n",
        "  for cluster_b in period_b:\n",
        "    print(cluster_a)\n",
        "    print(cluster_b)\n",
        "  break;"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['devops', 'security', 'scrum', 'microservices', 'data-science']\n",
            "['sqlite', 'continuous-integration', 'scripting-language']\n",
            "['devops', 'security', 'scrum', 'microservices', 'data-science']\n",
            "['autosar', 'software-design']\n",
            "['devops', 'security', 'scrum', 'microservices', 'data-science']\n",
            "['.net-framework', '.net', 'sql', 'groovy', 'perl']\n",
            "['devops', 'security', 'scrum', 'microservices', 'data-science']\n",
            "['robotics', 'devops', 'linux', 'microservices']\n",
            "['devops', 'security', 'scrum', 'microservices', 'data-science']\n",
            "['oracle', 'matlab', 'json', 'xml', 'gradle', 'database', 'image-processing']\n",
            "['devops', 'security', 'scrum', 'microservices', 'data-science']\n",
            "['mongodb', 'simulation', 'hardware', 'scada', 'design']\n",
            "['devops', 'security', 'scrum', 'microservices', 'data-science']\n",
            "['extreme-programming', 'scrum']\n",
            "['devops', 'security', 'scrum', 'microservices', 'data-science']\n",
            "['lucene', 'architect', 'data-science', 'big-data']\n",
            "['devops', 'security', 'scrum', 'microservices', 'data-science']\n",
            "['artificial-intelligence', 'product-management', 'javascript', 'data-warehouse', 'jira', 'html', 'data-mining']\n",
            "['devops', 'security', 'scrum', 'microservices', 'data-science']\n",
            "['data-analysis', 'business-intelligence', 'software-architecture', 'cloud']\n",
            "['devops', 'security', 'scrum', 'microservices', 'data-science']\n",
            "['linked-data', 'relational-database', 'nosql', 'project-management']\n",
            "['devops', 'security', 'scrum', 'microservices', 'data-science']\n",
            "['monitoring', 'version-control', 'git', 'ansible', 'elasticsearch']\n",
            "['devops', 'security', 'scrum', 'microservices', 'data-science']\n",
            "['html5', 'amazon-web-services']\n",
            "['devops', 'security', 'scrum', 'microservices', 'data-science']\n",
            "['sharepoint', 'refactoring', 'software-engineering', 'ios']\n",
            "['devops', 'security', 'scrum', 'microservices', 'data-science']\n",
            "['php', 'c++', 'unit-testing', 'mysql', 'angularjs', 'symfony']\n",
            "['devops', 'security', 'scrum', 'microservices', 'data-science']\n",
            "['statistics', 'computer-science', 'data-conversion', 'compiler', 'user-experience', 'user-interface']\n",
            "['devops', 'security', 'scrum', 'microservices', 'data-science']\n",
            "['security', 'machine-learning', 'data-management', 'opengl', 'r']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXTGp3065sOq"
      },
      "source": [
        "# get exclusivity of a relation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7vLhJL0NZtOX"
      },
      "source": [
        "# def getWeightOfEdge(dataset, source, predicate, target):\n",
        "#   # calculate exclusivity\n",
        "#   outgoingPredicatesCount = dataset.query(\n",
        "#             'start == \"'+ source + '\" and relation == \"' + predicate + '\"', inplace = False).shape[0]\n",
        "#   incomingPredicatesCount = dataset.query(\n",
        "#             'end == \"'+ target + '\" and relation == \"' + predicate + '\"', inplace = False).shape[0]\n",
        "#   exclusivity = 1/(outgoingPredicatesCount + incomingPredicatesCount - 1)\n",
        "#   return exclusivity"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OVA89PDufKt4"
      },
      "source": [
        "# d = getWeightOfEdge(relation_Dataframe, 'http://dbpedia.org/resource/JavaScript', 'http://dbpedia.org/ontology/wikiPageWikiLink', 'http://dbpedia.org/resource/Google_Chrome')\n",
        "# d"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7GKWGviu7jXD"
      },
      "source": [
        "#  # Assign Exclusivity to Edge set \n",
        "#  edge_set_with_exclusivity = []\n",
        "#  for index, row in relation_Dataframe.iterrows():\n",
        "#    if row[\"start\"] != \"\" and row[\"relation\"] != \"\" and row[\"end\"] != \"\":\n",
        "#      e = getWeightOfEdge(relation_Dataframe, row[\"start\"], row[\"relation\"], row[\"end\"])\n",
        "#      edge_set_with_exclusivity.append([row[\"start\"], row[\"relation\"], row[\"end\"], e])\n",
        "#  edge_set_with_exclusivity_DF = pd.DataFrame(edge_set_with_exclusivity, columns= ['Start', 'Relation', 'End', 'Exclusivity']) \n",
        "#  edge_set_with_exclusivity_DF.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oxYTnYUhvc3Q"
      },
      "source": [
        "# edge_set_with_exclusivity_DF_filtered = edge_set_with_exclusivity_DF.copy()\n",
        "# edge_set_with_exclusivity_DF_filtered.shape\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s4-udHKYvhqT"
      },
      "source": [
        "# edge_set_with_exclusivity_DF_filtered = edge_set_with_exclusivity_DF_filtered.drop_duplicates()\n",
        "# edge_set_with_exclusivity_DF_filtered.head()\n",
        "# edge_set_with_exclusivity_DF_filtered.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BdD3DuHlGvpD"
      },
      "source": [
        "# Assign total weight of all paths in graphFull dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "enn2qW-9AZ4j"
      },
      "source": [
        "# graphWithPathWeight = []\n",
        "# for index, row in graphFull.iterrows():\n",
        "#   weight = 0\n",
        "#   total_rev_exclusivity = 0\n",
        "#   if row[\"node1\"] == '':\n",
        "#     df = edge_set_with_exclusivity_DF_filtered.query('Start == \"'+ row[\"source_node\"] + '\" and Relation == \"' + row[\"r1\"] + '\" and End == \"' + row[\"target_node\"] + '\"', inplace = False)\n",
        "#     total_rev_exclusivity = 1/ df[\"Exclusivity\"].values[0]\n",
        "#     weight = 1/ total_rev_exclusivity\n",
        "#     graphWithPathWeight.append([row[\"source_node\"], row[\"r1\"], \"\", \"\", \"\", \"\", row[\"target_node\"], weight])\n",
        "#   elif row[\"node1\"] != '' and row[\"node2\"] == '':\n",
        "#     df1 = edge_set_with_exclusivity_DF_filtered.query('Start == \"'+ row[\"source_node\"] + '\" and Relation == \"' + row[\"r1\"] + '\" and End == \"' + row[\"node1\"] + '\"', inplace = False)\n",
        "#     df2 = edge_set_with_exclusivity_DF_filtered.query('Start == \"'+ row[\"node1\"] + '\" and Relation == \"' + row[\"r2\"] + '\" and End == \"' + row[\"target_node\"] + '\"', inplace = False)\n",
        "#     total_rev_exclusivity = (1/df1[\"Exclusivity\"].values[0]) + (1/df2[\"Exclusivity\"].values[0])\n",
        "#     weight = 1/ total_rev_exclusivity\n",
        "#     graphWithPathWeight.append([row[\"source_node\"], row[\"r1\"], row[\"node1\"], row[\"r2\"], \"\", \"\", row[\"target_node\"], weight])\n",
        "#   else:\n",
        "#     df1 = edge_set_with_exclusivity_DF_filtered.query('Start == \"'+ row[\"source_node\"] + '\" and Relation == \"' + row[\"r1\"] + '\" and End == \"' + row[\"node1\"] + '\"', inplace = False)\n",
        "#     df2 = edge_set_with_exclusivity_DF_filtered.query('Start == \"'+ row[\"node1\"] + '\" and Relation == \"' + row[\"r2\"] + '\" and End == \"' + row[\"node2\"] + '\"', inplace = False)\n",
        "#     df3 = edge_set_with_exclusivity_DF_filtered.query('Start == \"'+ row[\"node2\"] + '\" and Relation == \"' + row[\"r3\"] + '\" and End == \"' + row[\"target_node\"] + '\"', inplace = False)\n",
        "#     total_rev_exclusivity = (1/df1[\"Exclusivity\"].values[0]) + (1/df2[\"Exclusivity\"].values[0]) + (1/df3[\"Exclusivity\"].values[0])\n",
        "#     weight = 1/ total_rev_exclusivity\n",
        "#     graphWithPathWeight.append([row[\"source_node\"], row[\"r1\"], row[\"node1\"], row[\"r2\"], row[\"node2\"], row[\"r3\"], row[\"target_node\"], weight])\n",
        "\n",
        "# graphFullWithWeights = pd.DataFrame(graphWithPathWeight, columns= ['source_node', 'r1', 'node1', 'r2', 'node2', 'r3', 'target_node', 'Weight'])\n",
        "# graphFullWithWeights.shape\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kV1z45SjFcic"
      },
      "source": [
        "# graphFullWithWeights.tail(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k617tcplIdmY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}