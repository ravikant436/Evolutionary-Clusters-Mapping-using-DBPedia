{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "AdUnZJRw-dcP",
        "outputId": "011beaf3-614f-4b1a-e809-836a76f80d1a"
      },
      "source": [
        "co_occurence_path_2016_t2 = pd.read_csv('/content/drive/My Drive/Colab Notebooks/Particle Density Algo/raw_data/Data/T8/edgelist-OktDes2017-10countries.csv', header=None)\n",
        "occurence_path_2016_t2 = pd.read_csv('/content/drive/My Drive/Colab Notebooks/Particle Density Algo/raw_data/Data/T8/node-OktDes2017-10countries.csv', header=None)\n",
        "occurence_path_2016_t2[[0, 1, 2]]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>database</td>\n",
              "      <td>291</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>python</td>\n",
              "      <td>3555</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>javascript</td>\n",
              "      <td>1142</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>r</td>\n",
              "      <td>514</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>big-data</td>\n",
              "      <td>1154</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1901</th>\n",
              "      <td>linked-data</td>\n",
              "      <td>2</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1902</th>\n",
              "      <td>scada</td>\n",
              "      <td>1</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1903</th>\n",
              "      <td>smpp</td>\n",
              "      <td>1</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1904</th>\n",
              "      <td>cuda</td>\n",
              "      <td>1</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1905</th>\n",
              "      <td>debian</td>\n",
              "      <td>1</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1906 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                0     1      2\n",
              "0        database   291   True\n",
              "1          python  3555   True\n",
              "2      javascript  1142   True\n",
              "3               r   514   True\n",
              "4        big-data  1154   True\n",
              "...           ...   ...    ...\n",
              "1901  linked-data     2  False\n",
              "1902        scada     1  False\n",
              "1903         smpp     1  False\n",
              "1904         cuda     1  False\n",
              "1905       debian     1  False\n",
              "\n",
              "[1906 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dac8M27H1mQh"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "import seaborn as sns\n",
        "import json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2-Hmc3lkrtNy"
      },
      "source": [
        "# Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rcd5CSGrtPd6"
      },
      "source": [
        "def removeHyperLink(s):\n",
        "  return s.rsplit('/', 1)[-1]\n",
        "\n",
        "def removeWhiteSpaces(s):\n",
        "  return s.strip()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZgOPWtuFAXN"
      },
      "source": [
        "#Similarity Formula Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ylbCh2L-z8qJ"
      },
      "source": [
        "**prepare unique pairs of nodes**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3KJq7GUdz8Pj"
      },
      "source": [
        "def getUniquePairOfNodes(df):\n",
        "  node_pair_list = []\n",
        "  for index, row in df.iterrows():\n",
        "    temp_l = [row[\"skill_a\"], row[\"skill_b\"]]\n",
        "    node_pair_list.append(temp_l)\n",
        "  new_list = []\n",
        "  temp_list = node_pair_list.copy()\n",
        "  for item in temp_list:\n",
        "    item.sort()\n",
        "    if item not in new_list:\n",
        "      new_list.append(item)\n",
        "  node_pair_list = new_list\n",
        "  result_df = pd.DataFrame(node_pair_list, columns=['node_a', 'node_b'])\n",
        "  return result_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ZZGo4QTFaS_"
      },
      "source": [
        "**Get Neigbours of node**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_RsDaH3tFwCX"
      },
      "source": [
        "def getAllUniqueNodes(df):\n",
        "  skill_a_list = list(df[\"skill_a\"])\n",
        "  skill_b_list = list(df[\"skill_b\"])\n",
        "  all_nodes = skill_a_list + skill_b_list\n",
        "  unique_nodes = list(set(all_nodes))\n",
        "  return unique_nodes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aVP28IoMBUiQ"
      },
      "source": [
        "def getNeigboursOfNode(df,node):\n",
        "  neighbours = [node]\n",
        "  left_n = list(df.query('skill_a == \"' + node + '\"')[\"skill_b\"])\n",
        "  right_n = list(df.query('skill_b == \"' + node + '\"')[\"skill_a\"])\n",
        "  all_neighbours = left_n + right_n + neighbours\n",
        "  all_neighbours = list(set(all_neighbours))\n",
        "  return all_neighbours"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OSwBFSVDJaYz"
      },
      "source": [
        "**Prepare dataframe with unique nodes and their neighbours**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PjN3KwIEMxaG"
      },
      "source": [
        "def createNeighbourDataSet(df):\n",
        "  unique_nodes = getAllUniqueNodes(df)\n",
        "  nodes_with_neighbours = []\n",
        "  for node in unique_nodes:\n",
        "    neighbours = getNeigboursOfNode(df,node)\n",
        "    temp_l = [node, neighbours, len(neighbours)]\n",
        "    nodes_with_neighbours.append(temp_l)\n",
        "  neighbour_df = pd.DataFrame(nodes_with_neighbours, columns=['Node', 'Neighbours', 'N_len'])\n",
        "  return neighbour_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q3fknInezWIP"
      },
      "source": [
        "**Get Neighbour Count**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6LqMWC_NzaBP"
      },
      "source": [
        "def getNeighbourCount(node, df):\n",
        "  n_count = list(df.query('Node == \"' + node + '\"')[\"N_len\"])\n",
        "  if n_count:\n",
        "    n_count = n_count[0]\n",
        "  else:\n",
        "    n_count = 0\n",
        "  return n_count"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KiH3bmRbOC83"
      },
      "source": [
        "**Get Common neighbour nodes**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rRlnXdapLJo6"
      },
      "source": [
        "def getCommonNeighbourNodes(node1, node2, df):\n",
        "  n1 = list(df.query('Node == \"' + node1 + '\"')[\"Neighbours\"])\n",
        "  if n1:\n",
        "    n1 = n1[0]\n",
        "  else:\n",
        "    n1 = []\n",
        "  n2 = list(df.query('Node == \"' + node2 + '\"')[\"Neighbours\"])\n",
        "  if n2:\n",
        "    n2 = n2[0]\n",
        "  else:\n",
        "    n2 = []\n",
        "  n1_set = set(n1) \n",
        "  n2_set = set(n2) \n",
        "\n",
        "  if (n1_set & n2_set):\n",
        "    return list(n1_set & n2_set)\n",
        "  else:\n",
        "    return []  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qIvHZzY7AWZ"
      },
      "source": [
        "#calculate Similarity 𝜎(v,w)\n",
        "\n",
        "E(ij) = [C(ij)]^2 / [C(i)*C(j)]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-rrY-trekoU"
      },
      "source": [
        "def calculateSigmaTwoNodes(df_co_occ, df_occ, df, n1, n2):\n",
        "  c_i_j1 = df_co_occ.query('skill_a == \"' + n1 + '\" and skill_b == \"' + n2 + '\"')[[\"co_occurence\"]]\n",
        "  c_i_j2 = df_co_occ.query('skill_a == \"' + n2 + '\" and skill_b == \"' + n1 + '\"')[[\"co_occurence\"]]\n",
        "  c_i_j = c_i_j1.append(c_i_j2, ignore_index=False)\n",
        "  c_i_j = c_i_j.drop_duplicates()\n",
        "  c_i_j_sum = c_i_j[\"co_occurence\"].sum()\n",
        "\n",
        "  # # get occurenct of c_i and c_j\n",
        "  c_i = df_occ.query('skill == \"' + n1 + '\"')[[\"occurence\"]]\n",
        "  c_i = c_i.drop_duplicates()\n",
        "  c_i_sum = c_i[\"occurence\"].sum()\n",
        "  c_j = df_occ.query('skill == \"' + n2 + '\"')[[\"occurence\"]]\n",
        "  c_j = c_j.drop_duplicates()\n",
        "  c_j_sum = c_j[\"occurence\"].sum()\n",
        "\n",
        "  # calculate equivalence index\n",
        "  sigma = (np.square(c_i_j_sum))/ (c_i_sum * c_j_sum)\n",
        "  \n",
        "  return sigma"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wO2-pn-kAL40"
      },
      "source": [
        "# calculateSigmaTwoNodes(df_t1_de, df_t1_occ_de, df_t1_de_neighbours, 'javascript', 'selenium')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7MUarq6y6_8l"
      },
      "source": [
        "# def calculateSigmaTwoNodes(df, n1, n2):\n",
        "#   common_nodes = len(getCommonNeighbourNodes(n1, n2, df))\n",
        "#   neighbours_n1_count = getNeighbourCount(n1, df)\n",
        "#   neighbours_n2_count = getNeighbourCount(n2, df)\n",
        "#   if common_nodes > 0:\n",
        "#     sigma = common_nodes/np.sqrt(neighbours_n1_count*neighbours_n2_count)\n",
        "#   else:\n",
        "#     sigma = 0\n",
        "#   return sigma"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hTJdHTEkPZmW"
      },
      "source": [
        "def dataPreparation(co_occurence_data, occurence_data, country_code):\n",
        "  co_occ_colnames=['skill_a', 'skill_b', 'co_occurence', 'A', 'B', 'C', 'D', 'E', 'time', 'country']\n",
        "  occ_col_names = ['skill', 'occurence', 'A', 'B', 'country','C']\n",
        "  co_occ_df = pd.read_csv(co_occurence_data, names=co_occ_colnames, header=None)\n",
        "  occ_df = pd.read_csv(occurence_data, names=occ_col_names, header=None)\n",
        "\n",
        "  # co_occ_df['skill_a'] = co_occ_df['skill_a'].apply(removeHyperLink).apply(removeWhiteSpaces)\n",
        "  # co_occ_df['skill_b'] = co_occ_df['skill_b'].apply(removeHyperLink).apply(removeWhiteSpaces)\n",
        "  # co_occ_df['country'] = co_occ_df['country'].apply(removeWhiteSpaces)\n",
        "\n",
        "  # occ_df['skill'] = occ_df['skill'].apply(removeHyperLink).apply(removeWhiteSpaces)\n",
        "  # occ_df['country'] = occ_df['country'].apply(removeWhiteSpaces)\n",
        "  \n",
        "  # filter only country rows\n",
        "  co_occ_df = co_occ_df.query('country == \"' + country_code + '\"')\n",
        "  occ_df = occ_df.query('country == \"' + country_code + '\"')\n",
        "\n",
        "  # get all unique nodes\n",
        "  unique_nodes_pairs = getUniquePairOfNodes(co_occ_df)\n",
        "  \n",
        "  # get Neigbours\n",
        "  df_neighbours = createNeighbourDataSet(co_occ_df)\n",
        "\n",
        "  # Sigma calculation\n",
        "  node_pair_similarity = []\n",
        "  for index, row in unique_nodes_pairs.iterrows():\n",
        "    temp_sigma = calculateSigmaTwoNodes(co_occ_df, occ_df, df_neighbours, row[\"node_a\"], row[\"node_b\"])\n",
        "    temp_list = [row[\"node_a\"], row[\"node_b\"], temp_sigma]\n",
        "    node_pair_similarity.append(temp_list)\n",
        "  node_pair_with_sigma = pd.DataFrame(node_pair_similarity, columns=[\"node_a\", \"node_b\", \"sigma\"])\n",
        "  \n",
        "  return co_occ_df, occ_df, df_neighbours, unique_nodes_pairs, node_pair_with_sigma"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5TLBXS0-kpn"
      },
      "source": [
        "#Similarity calculation with temporal Smoothness for T2 and after\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sKvUk1q7eX_X"
      },
      "source": [
        "def applyTemporalSmoothing(node_pair_with_sigma_t2, node_pair_with_sigma_t1, alpha):\n",
        "  result_list = []\n",
        "  for index, row in node_pair_with_sigma_t2.iterrows():\n",
        "    node_a = row[\"node_a\"]\n",
        "    node_b = row[\"node_b\"]\n",
        "    sigma_t2 = row[\"sigma\"]\n",
        "    # get sigma_t1 for node pair, if it exist then good else keep sigma_t1 = 0\n",
        "    sigma_t1 = list(node_pair_with_sigma_t1.query('node_a==\"' + node_a + '\" and node_b==\"' + node_b + '\" or node_a==\"' + node_b + '\" and node_b==\"' + node_a + '\"')[\"sigma\"])\n",
        "    if not sigma_t1:\n",
        "      sigma_t1_val = 0.0\n",
        "    else:\n",
        "      sigma_t1_val = sigma_t1[0]\n",
        "    sigma_final = alpha*(sigma_t2 - sigma_t1_val) + sigma_t1_val\n",
        "    result_list.append([node_a, node_b, sigma_final])\n",
        "  result_df = pd.DataFrame(result_list, columns=['node_a', 'node_b', 'sigma'])\n",
        "  return result_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vI0S7WfB8N7W"
      },
      "source": [
        "**get Epsilon Neighbourhood of all the nodes**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3-sfgzxPXh_P"
      },
      "source": [
        "def getEpsilonNeighbourhood(df, sigma_df, eps):\n",
        "  eps_neigbours = []\n",
        "  for index, row in df.iterrows():\n",
        "    neighbours = row[\"Neighbours\"]\n",
        "    eps_neigbours_of_node = []\n",
        "    for item in neighbours:\n",
        "      #find similarity distance between node and neighbour node\n",
        "      if item != row[\"Node\"]:\n",
        "        similarity = list(sigma_df.query('node_a == \"'+ row[\"Node\"] + '\" and node_b == \"' + item + '\"')[\"sigma\"])\n",
        "        if not similarity:\n",
        "          similarity = list(sigma_df.query('node_b == \"'+ row[\"Node\"] + '\" and node_a == \"' + item + '\"')[\"sigma\"])\n",
        "        if similarity[0] >= eps:\n",
        "          eps_neigbours_of_node.append(item)\n",
        "    eps_neigbours.append([row[\"Node\"], row[\"Neighbours\"], row[\"N_len\"], eps_neigbours_of_node, len(eps_neigbours_of_node)])\n",
        "  df = pd.DataFrame(eps_neigbours, columns=[\"Node\", \"Neighbours\", \"N_len\", \"Eps_Neighbour\", \"Eps_N_len\"])\n",
        "  return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yk1KAM0-6rDo"
      },
      "source": [
        "# DBScan Clustering Process "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a9rDbB119BcL"
      },
      "source": [
        "def getCoreNodes(df_eps_neighbour, min_points):\n",
        "  # finding core points\n",
        "  core_points = df_eps_neighbour[df_eps_neighbour.Eps_N_len >= min_points]\n",
        "  core_points = list(core_points[\"Node\"])\n",
        "  return core_points"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rlJbjY5oAHw3"
      },
      "source": [
        "\n",
        "**Directly Reachable Nodes**\n",
        "\n",
        "x is directly reachable from v (core node) if x is in eps-neighbourhood of v\n",
        "\n",
        "**Reachable Nodes**\n",
        "\n",
        "Node y is reachable from node x, "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SBDuk9sryq0J"
      },
      "source": [
        "def directlyReachableFromCoreNodes(df, coreNodes, node):  #input - any node\n",
        "  result = []\n",
        "  for index, row in df.iterrows():\n",
        "    if row[\"Node\"] in coreNodes:\n",
        "      temp_list = row[\"Eps_Neighbour\"]\n",
        "      if node in temp_list:\n",
        "        result.append(row[\"Node\"])\n",
        "  return result                             #output - core nodes to which input node is directly reachable from"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sg8IrMoCqzTM"
      },
      "source": [
        "def reachableFromCoreNode(df, coreNodes, allNodes):  #input - core nodes\n",
        "  result_list = []\n",
        "  for node in allNodes:\n",
        "    for core_node in coreNodes:\n",
        "      if core_node != node:\n",
        "        isReachable = isReachableFromCoreNode(df, node, core_node, coreNodes)\n",
        "        if isReachable == 1: \n",
        "          temp_list = [node, core_node, isReachable]\n",
        "          result_list.append(temp_list)\n",
        "  result_df = pd.DataFrame(result_list, columns=[\"Node\", \"Core_node\", \"is_Reachable\"])\n",
        "  return result_df\n",
        "\n",
        "def isReachableFromCoreNode(df, node, core_node, coreNodes):\n",
        "  active_nodes = [node]\n",
        "  seen_nodes = []\n",
        "  while True:\n",
        "    # print('Active:', active_nodes)\n",
        "    # print('Seen:', seen_nodes)\n",
        "    if active_nodes:\n",
        "      n = active_nodes.pop(0)\n",
        "      # print('processing:', n)\n",
        "      seen_nodes.append(n)\n",
        "      temp_DRN = directlyReachableFromCoreNodes(df, coreNodes, n)\n",
        "      if core_node in temp_DRN:\n",
        "        # print(\"bingo\")\n",
        "        # print(temp_DRN)\n",
        "        return 1\n",
        "      for i in temp_DRN:\n",
        "        if i not in seen_nodes and i not in active_nodes:\n",
        "          active_nodes.append(i)\n",
        "    else:\n",
        "      return 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgyC40T7a78R"
      },
      "source": [
        "**Connected Nodes**\n",
        "\n",
        "A node **v** is connected to node **w** if there is a node **x** such that both **v** and **w** are reachable from **x**\n",
        "\n",
        "fyi : node is reachable from a core node"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YRkTDC2RYoq_"
      },
      "source": [
        "def connectedNodes(df, all_nodes):\n",
        "  result_list = []\n",
        "  for i in all_nodes:\n",
        "    for j in all_nodes:\n",
        "      if i != j:\n",
        "        ic = isConnectedNodes(df, i, j)\n",
        "        if ic == 1:\n",
        "          temp_list = [i, j, ic]\n",
        "          result_list.append(temp_list)\n",
        "  result_df = pd.DataFrame(result_list, columns=[\"A\", \"B\", \"is_connected\"])\n",
        "  return result_df\n",
        "\n",
        "def isConnectedNodes(df, a, b):\n",
        "  # get nodes from which a is reachable\n",
        "  a_reachable = list(df.query('Node == \"'+ a + '\" and is_Reachable == \"1\"', inplace = False)[\"Core_node\"])\n",
        "  # get nodes from which b is reachable\n",
        "  b_reachable = list(df.query('Node == \"'+ b + '\" and is_Reachable == \"1\"', inplace = False)[\"Core_node\"])\n",
        "  # get common\n",
        "  a_set = set(a_reachable)\n",
        "  b_set = set(b_reachable)\n",
        "\n",
        "  if (a_set & b_set):\n",
        "    return 1 \n",
        "  else:\n",
        "    return 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a3uyAFKjYAsj"
      },
      "source": [
        "def isConnected(df, n1, n2):\n",
        "  x = len(list(df.query('A == \"'+ n1 + '\" and B == \"' + n2 + '\"', inplace = False)[\"is_connected\"]))\n",
        "  y = len(list(df.query('A == \"'+ n2 + '\" and B == \"' + n1 + '\"', inplace = False)[\"is_connected\"]))\n",
        "  if x >= 1 or y >= 1:\n",
        "    return True\n",
        "  else:\n",
        "    return False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "opzcUb3wVVZG"
      },
      "source": [
        "#Clustering-Function\n",
        "\n",
        "Rule 1: if **v** is in **S** and **w** is reachable from v, then w is also in **S**\n",
        "\n",
        "Rule 2: for all v, w belongs to **S**, v is connected to w"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hRux2OpXV6ll"
      },
      "source": [
        "def ClusteringProcess(eps, min_points, df_neighbours, node_pair_with_sigma):\n",
        "  # get Epsilon Neighbours\n",
        "  df_eps_neighbours = getEpsilonNeighbourhood(df_neighbours, node_pair_with_sigma, eps)\n",
        "  # all node list\n",
        "  all_nodes = list(df_eps_neighbours[\"Node\"])\n",
        "\n",
        "  # get core nodes\n",
        "  core_nodes = getCoreNodes(df_eps_neighbours, min_points)\n",
        "  \n",
        "  # get Non-core nodes\n",
        "  non_core_nodes = []\n",
        "  for i in all_nodes:\n",
        "    if i not in core_nodes:\n",
        "      non_core_nodes.append(i)\n",
        "\n",
        "  # get Reachable nodes DF\n",
        "  reachableNodesDF = reachableFromCoreNode(df_eps_neighbours, core_nodes, all_nodes)\n",
        "  # get connected nodes DF\n",
        "  connected_node_df = connectedNodes(reachableNodesDF, all_nodes)\n",
        "\n",
        "  # # Making copies\n",
        "  temp_core_nodes = core_nodes.copy()\n",
        "  temp_non_core_nodes = non_core_nodes.copy()\n",
        "  temp_all_nodes = all_nodes.copy()\n",
        "\n",
        "  # # Clustering Process\n",
        "  clusters = []\n",
        "  while len(temp_core_nodes) > 0:\n",
        "    temp_cluster = []\n",
        "    core = temp_core_nodes.pop(0)\n",
        "    \n",
        "    # Intialize core with core element\n",
        "    temp_cluster.append(core)\n",
        "    # Add elements which are reachable from core element\n",
        "    unprocessed_nodes = []\n",
        "    rchbl = list(reachableNodesDF.query('Core_node == \"'+ core + '\"', inplace = False)[\"Node\"])\n",
        "    \n",
        "    # processed_nodes.append(core)\n",
        "    unprocessed_nodes = unprocessed_nodes + rchbl\n",
        "    temp_cluster = temp_cluster + rchbl\n",
        "    #No need to check connectivity for now; as there is only one core node and nodes reachable from it\n",
        "    while len(unprocessed_nodes) > 0:\n",
        "      k = unprocessed_nodes.pop(0)\n",
        "      if k in core_nodes:\n",
        "        # find it's reachable nodes\n",
        "        temp_rchbl = list(reachableNodesDF.query('Core_node == \"'+ k + '\"', inplace = False)[\"Node\"])\n",
        "        # check the connectivity of these temp_rchbl nodes from all the nodes present in temp_cluster\n",
        "        for nod in temp_rchbl:\n",
        "          not_connected = 0\n",
        "          for cn in temp_cluster:\n",
        "            isConct = isConnected(connected_node_df, nod, cn)\n",
        "            if isConct == False:\n",
        "              not_connected = 1\n",
        "          if not_connected == 0:\n",
        "            temp_cluster.append(nod)\n",
        "            unprocessed_nodes.append(nod) # adding as we need to process it's reachable nodes too\n",
        "\n",
        "    # now let's remove the nodes of temp_cluster from core_node and all_node list\n",
        "    for nod in temp_cluster:\n",
        "      if nod in temp_core_nodes:\n",
        "        temp_core_nodes.remove(nod)\n",
        "      if nod in temp_all_nodes:\n",
        "        temp_all_nodes.remove(nod)\n",
        "    # cluster is ready\n",
        "    clusters.append(temp_cluster)\n",
        "  noise_nodes = temp_all_nodes.copy()\n",
        "  return clusters, noise_nodes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQd3BjCcdEiv"
      },
      "source": [
        "#Modularity (Q_mid) calculation for EPs optimization\n",
        "\n",
        "![image.png](data:image/png;base64,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)\n",
        "\n",
        "where NC is the number of clusters, TS the total simi-\n",
        "larity between all pairs of nodes in the graph, ISc the total\n",
        "similarity of a pair of nodes within a cluster c, DSc the total\n",
        "similarity between a node in the cluster c and any node in\n",
        "the graph."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JmbsGvB18xoZ"
      },
      "source": [
        "def getModularity(df, clusters):\n",
        "  modularity = 0\n",
        "  TS = df.sum(axis = 0, skipna = True).sigma\n",
        "  for cluster in clusters:\n",
        "    internal_cluster_edges = getClusterInternalEdges(df, cluster)\n",
        "    ISc = internal_cluster_edges.sum(axis = 0, skipna = True).sigma\n",
        "\n",
        "    all_cluster_edges = getClusterAllEdges(df, cluster)\n",
        "    DSc = all_cluster_edges.sum(axis = 0, skipna = True).sigma\n",
        "\n",
        "    sum_value = (ISc/TS) - np.square(DSc/TS)\n",
        "    \n",
        "    modularity += sum_value\n",
        "\n",
        "  return modularity"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FsZRlWoqdGWx"
      },
      "source": [
        "def getClusterInternalEdges(df, cluster):\n",
        "  return df[df['node_a'].isin(cluster) & df['node_b'].isin(cluster)]\n",
        "\n",
        "def getClusterAllEdges(df, cluster):\n",
        "  tmp_all_1 = df[df['node_a'].isin(cluster)] \n",
        "  tmp_all_2 = df[df['node_b'].isin(cluster)]\n",
        "  tmp_all = pd.concat([tmp_all_1, tmp_all_2])\n",
        "  tmp_all.reset_index(drop=True, inplace=True)\n",
        "  tmp_all = tmp_all.drop_duplicates()\n",
        "  return tmp_all\n",
        "\n",
        "def getClusterExternalEdges(df, cluster):\n",
        "  internal_edges = getClusterInternalEdges(df, cluster)\n",
        "  all_edges = getClusterAllEdges(df, cluster)\n",
        "  dfs_dictionary = {'DF1':internal_edges,'DF2':all_edges}\n",
        "  external_edges = pd.concat(dfs_dictionary)\n",
        "  external_edges = external_edges.drop_duplicates(keep=False)\n",
        "  return external_edges"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vIaCnyFEjFGz"
      },
      "source": [
        "# Optimization of Eps"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pzdMyw4kfwbf"
      },
      "source": [
        "def findBestEps(mean_eps, df_neighbours, node_pair_with_sigma):\n",
        "  min_points_list = [2]\n",
        "  step_size = 0.02\n",
        "  # start_eps = mean_eps - .10\n",
        "  # end_eps = mean_eps + 0.10\n",
        "  start_eps = 0.30\n",
        "  end_eps = 0.34\n",
        "  if start_eps < 0.05:\n",
        "    diff = 0.05 - start_eps\n",
        "    start_eps = .02\n",
        "    end_eps = end_eps + diff\n",
        "  start_eps = float(\"{:.2f}\".format(start_eps))\n",
        "  end_eps = float(\"{:.2f}\".format(end_eps))\n",
        "  modu_list = []\n",
        "  all_eps_list = []\n",
        "  clusters_res_list = []\n",
        "  noise_res_list = []\n",
        "  for eps in tqdm(np.arange(start_eps, end_eps, step_size)):\n",
        "    for min_point in min_points_list:\n",
        "      print(\"processing - \", eps)\n",
        "      print(\"min_point - \", min_point)\n",
        "      temp_clusters, temp_noise_nodes = ClusteringProcess(eps, min_point, df_neighbours, node_pair_with_sigma)\n",
        "      temp_Q_mid = getModularity(node_pair_with_sigma, temp_clusters)\n",
        "      all_eps_list.append(eps)\n",
        "      modu_list.append(temp_Q_mid)\n",
        "      clusters_res_list.append(temp_clusters)\n",
        "      noise_res_list.append(temp_noise_nodes)\n",
        "\n",
        "      print(\"Modularity=\",temp_Q_mid)\n",
        "      print(\"======================================\")\n",
        "\n",
        "  max_index = modu_list.index(max(modu_list))\n",
        "\n",
        "  print(modu_list)\n",
        "  print(\"Modularity=\",modu_list[max_index])\n",
        "  print(\"EPs=\", all_eps_list[max_index])\n",
        "  print(clusters_res_list[max_index])\n",
        "  print(\"======================================\")\n",
        "\n",
        "  return all_eps_list[max_index], clusters_res_list[max_index], noise_res_list[max_index], modu_list[max_index], all_eps_list, modu_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aNr6PXBKDNyM"
      },
      "source": [
        "**Prepare cluster for 2 consecutive timeframe**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yrbzfov8EZZD"
      },
      "source": [
        "# min_points = 2\n",
        "# alpha = 1\n",
        "# Network_nodes_df =  pd.DataFrame(columns=['node_id', 'Node'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OvAoI4g_n5cP"
      },
      "source": [
        "# get correct eps for different time frames"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VTFRG1Ggn484"
      },
      "source": [
        "# co_occurence_path_2016_t1 = '/content/drive/My Drive/Colab Notebooks/Particle Density Algo/raw_data/Data/T7/edgelist-JulSep2017.csv'\n",
        "# occurence_path_2016_t1 = '/content/drive/My Drive/Colab Notebooks/Particle Density Algo/raw_data/Data/T7/node-JulSep2017.csv'\n",
        "\n",
        "# co_occ_df_2016_t1, occ_df_2016_t1, df_neighbours_2016_t1, unique_nodes_pairs_2016_t1, node_pair_with_sigma_2016_t1 = dataPreparation(co_occurence_path_2016_t1, occurence_path_2016_t1, 'de')\n",
        "\n",
        "# mean_sigma_2016_t1 = node_pair_with_sigma_2016_t1[\"sigma\"].mean()\n",
        "# print(\"mean is \",mean_sigma_2016_t1)\n",
        "# mean_sigma_2016_t1 = float(\"{:.2f}\".format(mean_sigma_2016_t1))\n",
        "# optimal_eps_2016_t1, clusters_2016_t1, noise_nodes_2016_t1, modularity_2016_t1 = findBestEps(mean_sigma_2016_t1, min_points, df_neighbours_2016_t1, node_pair_with_sigma_2016_t1)\n",
        "# print(\"Best eps JulSep2017 = \", optimal_eps_2016_t1)\n",
        "\n",
        "######################\n",
        "\n",
        "# co_occurence_path_2016_t2 = '/content/drive/My Drive/Colab Notebooks/Particle Density Algo/raw_data/Data/T8/edgelist-OktDes2017-10countries.csv'\n",
        "# occurence_path_2016_t2 = '/content/drive/My Drive/Colab Notebooks/Particle Density Algo/raw_data/Data/T8/node-OktDes2017-10countries.csv'\n",
        "\n",
        "# co_occ_df_2016_t2, occ_df_2016_t2, df_neighbours_2016_t2, unique_nodes_pairs_2016_t2, node_pair_with_sigma_2016_t2 = dataPreparation(co_occurence_path_2016_t2, occurence_path_2016_t2, 'de')\n",
        "# mean_sigma_2016_t2 = node_pair_with_sigma_2016_t2[\"sigma\"].mean()\n",
        "# print(\"Mean is \", mean_sigma_2016_t2)\n",
        "# mean_sigma_2016_t2 = float(\"{:.2f}\".format(mean_sigma_2016_t2))\n",
        "# optimal_eps_2016_t2, clusters_2016_t2, noise_nodes_2016_t2, modularity_2016_t2 = findBestEps(mean_sigma_2016_t2, min_points, df_neighbours_2016_t2, node_pair_with_sigma_2016_t2)\n",
        "# print(\"Best eps octdec 2017 = \", optimal_eps_2016_t2)\n",
        "\n",
        "# node_pair_with_sigma_2016_t2_a = applyTemporalSmoothing(node_pair_with_sigma_2016_t2, node_pair_with_sigma_2016_t1, 0.2)\n",
        "# mean_sigma_2016_t2 = node_pair_with_sigma_2016_t2_a[\"sigma\"].mean()\n",
        "# mean_sigma_2016_t2 = float(\"{:.2f}\".format(mean_sigma_2016_t2))\n",
        "# optimal_eps_2016_t2, clusters_2016_t2, noise_nodes_2016_t2, modularity_2016_t2 = findBestEps(mean_sigma_2016_t2, min_points, df_neighbours_2016_t2, node_pair_with_sigma_2016_t2_a)\n",
        "# print(\"AprJun2016_0.2 = \", optimal_eps_2016_t2)\n",
        "\n",
        "# node_pair_with_sigma_2016_t2_b = applyTemporalSmoothing(node_pair_with_sigma_2016_t2, node_pair_with_sigma_2016_t1, 0.8)\n",
        "# mean_sigma_2016_t2 = node_pair_with_sigma_2016_t2_b[\"sigma\"].mean()\n",
        "# mean_sigma_2016_t2 = float(\"{:.2f}\".format(mean_sigma_2016_t2))\n",
        "# optimal_eps_2016_t2, clusters_2016_t2, noise_nodes_2016_t2, modularity_2016_t2 = findBestEps(mean_sigma_2016_t2, min_points, df_neighbours_2016_t2, node_pair_with_sigma_2016_t2_b)\n",
        "# print(\"AprJun2016_0.8 = \", optimal_eps_2016_t2)\n",
        "\n",
        "############################\n",
        "\n",
        "# co_occurence_path_2016_t3 = '/content/drive/My Drive/Colab Notebooks/Particle Density Algo/raw_data/Data/T3/edgelist-JulSep2016.csv'\n",
        "# occurence_path_2016_t3 = '/content/drive/My Drive/Colab Notebooks/Particle Density Algo/raw_data/Data/T3/node-JulSep2016.csv'\n",
        "\n",
        "# co_occ_df_2016_t3, occ_df_2016_t3, df_neighbours_2016_t3, unique_nodes_pairs_2016_t3, node_pair_with_sigma_2016_t3 = dataPreparation(co_occurence_path_2016_t3, occurence_path_2016_t3, 'de')\n",
        "\n",
        "# mean_sigma_2016_t3 = node_pair_with_sigma_2016_t3[\"sigma\"].mean()\n",
        "# mean_sigma_2016_t3 = float(\"{:.2f}\".format(mean_sigma_2016_t3))\n",
        "# optimal_eps_2016_t3, clusters_2016_t3, noise_nodes_2016_t3, modularity_2016_t3 = findBestEps(mean_sigma_2016_t3, min_points, df_neighbours_2016_t3, node_pair_with_sigma_2016_t3)\n",
        "# print(\"Best eps JULSEP2016 = \",optimal_eps_2016_t3)\n",
        "\n",
        "###########################\n",
        "\n",
        "# co_occurence_path_2016_t4 = '/content/drive/My Drive/Colab Notebooks/Particle Density Algo/raw_data/Data/T4/edgelist-OktDes2016.csv'\n",
        "# occurence_path_2016_t4 = '/content/drive/My Drive/Colab Notebooks/Particle Density Algo/raw_data/Data/T4/node-OktDes2016.csv'\n",
        "\n",
        "# co_occ_df_2016_t4, occ_df_2016_t4, df_neighbours_2016_t4, unique_nodes_pairs_2016_t4, node_pair_with_sigma_2016_t4 = dataPreparation(co_occurence_path_2016_t4, occurence_path_2016_t4, 'de')\n",
        "\n",
        "# mean_sigma_2016_t4 = node_pair_with_sigma_2016_t4[\"sigma\"].mean()\n",
        "# mean_sigma_2016_t4 = float(\"{:.2f}\".format(mean_sigma_2016_t4))\n",
        "# optimal_eps_2016_t4, clusters_2016_t4, noise_nodes_2016_t4, modularity_2016_t4 = findBestEps(mean_sigma_2016_t4, min_points, df_neighbours_2016_t4, node_pair_with_sigma_2016_t4)\n",
        "# print(\"Best eps OktDes2016 = \",optimal_eps_2016_t4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BrOmcdcZn-8n"
      },
      "source": [
        "# DE --- Data\n",
        "# JanMar2016 - (.09)\n",
        "# AprJun2016 - (.05)\n",
        "# JulSep2016 - (0.03)\n",
        "# OctDec2016 - 0.03)\n",
        "\n",
        "# JanMar2017 = .18\n",
        "# AprJun2017 = .13\n",
        "# JulSep2017 =  0.08\n",
        "# octdec2017 = 0.04\n",
        "\n",
        "# AprJun2017 : \n",
        "# with alpha = 0.2: 0.06\n",
        "# with alpha = 0.8: 0.10\n",
        "# with alpha = 0.0: 0.02"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vyrJtvwGDNBb"
      },
      "source": [
        "# co_occurence_path_t1 = '/content/drive/My Drive/Colab Notebooks/Particle Density Algo/raw_data/Data/T5/edgelist-JanMar2017.csv'\n",
        "# occurence_path_t1 = '/content/drive/My Drive/Colab Notebooks/Particle Density Algo/raw_data/Data/T5/node-JanMar2017.csv'\n",
        "\n",
        "# co_occ_df_t1, occ_df_t1, df_neighbours_t1, unique_nodes_pairs_t1, node_pair_with_sigma_t1 = dataPreparation(co_occurence_path_t1, occurence_path_t1, 'de')\n",
        "# mean_sigma_t1 = node_pair_with_sigma_t1[\"sigma\"].mean()\n",
        "# mean_sigma_t1 = float(\"{:.2f}\".format(mean_sigma_t1))\n",
        "\n",
        "# optimal_eps_t1, clusters_t1, noise_nodes_t1, modularity_t1 = findBestEps(mean_sigma_t1, min_points, df_neighbours_t1, node_pair_with_sigma_t1)\n",
        "\n",
        "# clusters_t1, noise_nodes_t1 = ClusteringProcess(.18, min_points, df_neighbours_t1, node_pair_with_sigma_t1)\n",
        "# Q_mid_t1 = getModularity(node_pair_with_sigma_t1, clusters_t1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M9YLwCF7RgpJ"
      },
      "source": [
        "mod_list = [0.4814832542137488, 0.47973244596406817, 0.47973244596406817, 0.47268146475578354, 0.47268146475578354, 0.49954880786787903, 0.4673588006442232, 0.4673588006442232, 0.4673588006442232, 0.39568383618092207, 0.39568383618092207, 0.395683836180922, 0.395683836180922, 0.2826571617002696, 0.2355916219289209, 0.2355916219289209, 0.2355916219289209, 0.2355916219289209, 0.2355916219289209, 0.2355916219289209]\n",
        "eps_list = [0.08, 0.1, 0.12, 0.14, 0.16, 0.18, 0.2, 0.22, 0.24, 0.26, 0.28, 0.3, 0.32, 0.34, 0.36, 0.38, 0.4, 0.42, 0.44, 0.46]\n",
        "\n",
        "# apr-jun list\n",
        "# mod_list = [-0.0024937824763165972, 0.005167372691938121, 0.022088611663185764, 0.18717919525545088, 0.18582416325944542, 0.48861735402314765, 0.4735277139390298, 0.39867596230098196, 0.39103693484309754, 0.3739253581169257, 0.3646501653694078, 0.35415805689145025]\n",
        "# eps_list = [0.02, 0.04, 0.06, 0.08, 0.1, 0.12, 0.14, 0.16, 0.18, 0.2, 0.22, 0.24]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gxeNzHPzSpl3"
      },
      "source": [
        "import seaborn as sns\n",
        "\n",
        "plt.figure(figsize=(8,8))\n",
        "sns.set_style('whitegrid')\n",
        "sns.lineplot(eps_list, mod_list, color='orange', linewidth=2.5)\n",
        "plt.xlabel('Eps')\n",
        "plt.ylabel('Modularity')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eDzWpBpt_tRo"
      },
      "source": [
        "**Sigma Distribution analysis**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FK9lz7HR_sKu"
      },
      "source": [
        "# all_sigma = list(node_pair_with_sigma_t1[\"sigma\"])\n",
        "# all_sigma = np.log(all_sigma)\n",
        "# plt.hist(all_sigma, color = 'blue', edgecolor = 'black', bins = 10)\n",
        "# node_pair_with_sigma_t1[\"sigma\"] = np.log(node_pair_with_sigma_t1[\"sigma\"])\n",
        "# node_pair_with_sigma_t1[\"sigma\"].describe()\n",
        "# lower = node_pair_with_sigma_t1[\"sigma\"].quantile(.25)\n",
        "# upper = node_pair_with_sigma_t1[\"sigma\"].quantile(.75)\n",
        "# (upper-lower)/10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qSFZGDjxryhD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 989
        },
        "outputId": "602480cb-eb28-4abc-ad4f-bcf542557f04"
      },
      "source": [
        "co_occurence_path_t1 = '/content/drive/My Drive/Colab Notebooks/Particle Density Algo/raw_data/Data/T5/edgelist-JanMar2017.csv'\n",
        "occurence_path_t1 = '/content/drive/My Drive/Colab Notebooks/Particle Density Algo/raw_data/Data/T5/node-JanMar2017.csv'\n",
        "\n",
        "co_occ_df_t1, occ_df_t1, df_neighbours_t1, unique_nodes_pairs_t1, node_pair_with_sigma_t1 = dataPreparation(co_occurence_path_t1, occurence_path_t1, 'de')\n",
        "node_pair_with_sigma_t1.head(30)\n",
        "# # mean_sigma_t1 = node_pair_with_sigma_t1[\"sigma\"].mean()\n",
        "# # mean_sigma_t1 = float(\"{:.2f}\".format(mean_sigma_t1))\n",
        "# # optimal_eps_t1, clusters_t1, noise_nodes_t1, modularity_t1 = findBestEps(mean_sigma_t1, min_points, df_neighbours_t1, node_pair_with_sigma_t1)\n",
        "\n",
        "# clusters_t1, noise_nodes_t1 = ClusteringProcess(.18, min_points, df_neighbours_t1, node_pair_with_sigma_t1)\n",
        "# Q_mid_t1 = getModularity(node_pair_with_sigma_t1, clusters_t1)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>node_a</th>\n",
              "      <th>node_b</th>\n",
              "      <th>sigma</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>javascript</td>\n",
              "      <td>selenium</td>\n",
              "      <td>0.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>business-intelligence</td>\n",
              "      <td>statistics</td>\n",
              "      <td>0.000665</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>business-intelligence</td>\n",
              "      <td>security</td>\n",
              "      <td>0.000818</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>data-analysis</td>\n",
              "      <td>statistics</td>\n",
              "      <td>0.017857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>design</td>\n",
              "      <td>user-experience</td>\n",
              "      <td>0.052632</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>big-data</td>\n",
              "      <td>data-science</td>\n",
              "      <td>0.050277</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>devops</td>\n",
              "      <td>security</td>\n",
              "      <td>0.111888</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>big-data</td>\n",
              "      <td>devops</td>\n",
              "      <td>0.077700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>compiler</td>\n",
              "      <td>data-science</td>\n",
              "      <td>0.058824</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>big-data</td>\n",
              "      <td>business-intelligence</td>\n",
              "      <td>0.596563</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>architect</td>\n",
              "      <td>big-data</td>\n",
              "      <td>0.060779</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>microservices</td>\n",
              "      <td>security</td>\n",
              "      <td>0.123077</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>git</td>\n",
              "      <td>relational-database</td>\n",
              "      <td>0.333333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>database</td>\n",
              "      <td>postgresql</td>\n",
              "      <td>0.166667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>big-data</td>\n",
              "      <td>data-management</td>\n",
              "      <td>0.284024</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>.net-framework</td>\n",
              "      <td>ansible</td>\n",
              "      <td>0.250000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>big-data</td>\n",
              "      <td>software-engineering</td>\n",
              "      <td>0.002137</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>database</td>\n",
              "      <td>oracle</td>\n",
              "      <td>0.250000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>big-data</td>\n",
              "      <td>compiler</td>\n",
              "      <td>0.015195</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>.net</td>\n",
              "      <td>.net-framework</td>\n",
              "      <td>0.750000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>elasticsearch</td>\n",
              "      <td>machine-learning</td>\n",
              "      <td>0.250000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>data-mining</td>\n",
              "      <td>machine-learning</td>\n",
              "      <td>0.125000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>big-data</td>\n",
              "      <td>microservices</td>\n",
              "      <td>0.085470</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>business-intelligence</td>\n",
              "      <td>data-management</td>\n",
              "      <td>0.353519</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>devops</td>\n",
              "      <td>microservices</td>\n",
              "      <td>0.909091</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>data-analysis</td>\n",
              "      <td>machine-learning</td>\n",
              "      <td>0.071429</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>c++</td>\n",
              "      <td>oracle</td>\n",
              "      <td>0.023810</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>javascript</td>\n",
              "      <td>scripting-language</td>\n",
              "      <td>0.166667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>nosql</td>\n",
              "      <td>relational-database</td>\n",
              "      <td>0.333333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>architect</td>\n",
              "      <td>data-management</td>\n",
              "      <td>0.182336</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                   node_a                 node_b     sigma\n",
              "0              javascript               selenium  0.500000\n",
              "1   business-intelligence             statistics  0.000665\n",
              "2   business-intelligence               security  0.000818\n",
              "3           data-analysis             statistics  0.017857\n",
              "4                  design        user-experience  0.052632\n",
              "5                big-data           data-science  0.050277\n",
              "6                  devops               security  0.111888\n",
              "7                big-data                 devops  0.077700\n",
              "8                compiler           data-science  0.058824\n",
              "9                big-data  business-intelligence  0.596563\n",
              "10              architect               big-data  0.060779\n",
              "11          microservices               security  0.123077\n",
              "12                    git    relational-database  0.333333\n",
              "13               database             postgresql  0.166667\n",
              "14               big-data        data-management  0.284024\n",
              "15         .net-framework                ansible  0.250000\n",
              "16               big-data   software-engineering  0.002137\n",
              "17               database                 oracle  0.250000\n",
              "18               big-data               compiler  0.015195\n",
              "19                   .net         .net-framework  0.750000\n",
              "20          elasticsearch       machine-learning  0.250000\n",
              "21            data-mining       machine-learning  0.125000\n",
              "22               big-data          microservices  0.085470\n",
              "23  business-intelligence        data-management  0.353519\n",
              "24                 devops          microservices  0.909091\n",
              "25          data-analysis       machine-learning  0.071429\n",
              "26                    c++                 oracle  0.023810\n",
              "27             javascript     scripting-language  0.166667\n",
              "28                  nosql    relational-database  0.333333\n",
              "29              architect        data-management  0.182336"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-SKD8s8U_WV"
      },
      "source": [
        "# co_occurence_path_t2 = '/content/drive/My Drive/Colab Notebooks/Particle Density Algo/raw_data/Data/T6/edgelist-AprJun2017.csv'\n",
        "# occurence_path_t2 = '/content/drive/My Drive/Colab Notebooks/Particle Density Algo/raw_data/Data/T6/node-AprJun2017.csv'\n",
        "\n",
        "\n",
        "# co_occ_df_t2, occ_df_t2, df_neighbours_t2, unique_nodes_pairs_t2, node_pair_with_sigma_t2 = dataPreparation(co_occurence_path_t2, occurence_path_t2, 'de')\n",
        "# node_pair_with_sigma_t2 = applyTemporalSmoothing(node_pair_with_sigma_t2, node_pair_with_sigma_t1, alpha)\n",
        "\n",
        "# # mean_sigma_t2 = node_pair_with_sigma_t2[\"sigma\"].mean()\n",
        "# # mean_sigma_t2 = float(\"{:.2f}\".format(mean_sigma_t2))\n",
        "# # optimal_eps_t2, clusters_t2, noise_nodes_t2, modularity_t2 = findBestEps(mean_sigma_t2, min_points, df_neighbours_t2, node_pair_with_sigma_t2)\n",
        "\n",
        "# # .12 or .13\n",
        "# clusters_t2, noise_nodes_t2 = ClusteringProcess(.12, min_points, df_neighbours_t2, node_pair_with_sigma_t2)\n",
        "# Q_mid_t2 = getModularity(node_pair_with_sigma_t2, clusters_t2)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j6Au6yOA8bcC"
      },
      "source": [
        "# len(clusters_t2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "igaQorQYOODD"
      },
      "source": [
        "# co_occurence_path_t3 = '/content/drive/My Drive/Colab Notebooks/Particle Density Algo/raw_data/Data/T7/edgelist-JulSep2017.csv'\n",
        "# occurence_path_t3 = '/content/drive/My Drive/Colab Notebooks/Particle Density Algo/raw_data/Data/T7/node-JulSep2017.csv'\n",
        "\n",
        "# co_occ_df_t3, occ_df_t3, df_neighbours_t3, unique_nodes_pairs_t3, node_pair_with_sigma_t3 = dataPreparation(co_occurence_path_t3, occurence_path_t3, 'de')\n",
        "# node_pair_with_sigma_t3 = applyTemporalSmoothing(node_pair_with_sigma_t3, node_pair_with_sigma_t3, alpha)\n",
        "\n",
        "# mean_sigma_t3 = node_pair_with_sigma_t3[\"sigma\"].mean()\n",
        "# mean_sigma_t3 = float(\"{:.2f}\".format(mean_sigma_t3))\n",
        "# optimal_eps_t3, clusters_t3, noise_nodes_t3, modularity_t3 = findBestEps(mean_sigma_t3, min_points, df_neighbours_t3, node_pair_with_sigma_t3)\n",
        "\n",
        "# # .11\n",
        "# # clusters_t3, noise_nodes_t3 = ClusteringProcess(.11, min_points, df_neighbours_t3, node_pair_with_sigma_t3)\n",
        "# # Q_mid_t3 = getModularity(node_pair_with_sigma_t3, clusters_t3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2FDmGrG8jnEM"
      },
      "source": [
        "# co_occurence_path_t4 = '/content/drive/My Drive/Colab Notebooks/Particle Density Algo/raw_data/Data/T8/edgelist-OktDes2017-10countries.csv'\n",
        "# occurence_path_t4 = '/content/drive/My Drive/Colab Notebooks/Particle Density Algo/raw_data/Data/T8/node-OktDes2017-10countries.csv'\n",
        "\n",
        "# co_occ_df_t4, occ_df_t4, df_neighbours_t4, unique_nodes_pairs_t4, node_pair_with_sigma_t4 = dataPreparation(co_occurence_path_t4, occurence_path_t4, 'de')\n",
        "# node_pair_with_sigma_t4 = applyTemporalSmoothing(node_pair_with_sigma_t4, node_pair_with_sigma_t4, alpha)\n",
        "\n",
        "# # mean_sigma_t4 = node_pair_with_sigma_t4[\"sigma\"].mean()\n",
        "# # mean_sigma_t4 = float(\"{:.2f}\".format(mean_sigma_t4))\n",
        "# # print(\"Mean Sigma \", mean_sigma_t4)\n",
        "# # optimal_eps_t4, clusters_t4, noise_nodes_t4, modularity_t4 = findBestEps(mean_sigma_t4, min_points, df_neighbours_t4, node_pair_with_sigma_t4)\n",
        "\n",
        "# # .11\n",
        "# clusters_t4, noise_nodes_t4 = ClusteringProcess(.04, min_points, df_neighbours_t4, node_pair_with_sigma_t4)\n",
        "# Q_mid_t4 = getModularity(node_pair_with_sigma_t4, clusters_t4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "noGYGYJOO7Yk"
      },
      "source": [
        "# print(\"Modularity - \", Q_mid_t4)\n",
        "# clusters_t4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "blgmtTozWUK8"
      },
      "source": [
        "#Result File and Json file for Graph drawing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v-xzTw5edfph"
      },
      "source": [
        "def getLabelNode(edge_internal, edge_external, node_internal, node_external):\n",
        "  all_nodes = node_internal.split('#') + node_external.split('#')\n",
        "  all_nodes = [node for node in all_nodes if node]\n",
        "  edge_internal_list = edge_internal.split('\"')\n",
        "  edge_external_list = edge_external.split('\"')\n",
        "\n",
        "  # calculate n_cl_in and n_cl_ex\n",
        "  n_cl_in = 0\n",
        "  n_cl_ex = 0\n",
        "  for item in edge_internal_list:\n",
        "    if item:\n",
        "      n_cl_in = n_cl_in + float(item.split('#')[2])\n",
        "  for item in edge_external_list:\n",
        "    if item:\n",
        "      n_cl_ex = n_cl_ex + float(item.split('#')[2])\n",
        "  # calculate W_cl for all nodes\n",
        "  w_cl_list = []\n",
        "  for node in all_nodes:\n",
        "    k_cl = 0\n",
        "    for item in edge_internal_list:\n",
        "      temp_list = item.split('#')\n",
        "      if node in temp_list:\n",
        "        k_cl = k_cl + float(temp_list[2])\n",
        "    for item in edge_external_list:\n",
        "      temp_list = item.split('#')\n",
        "      if node in temp_list:\n",
        "        k_cl = k_cl + float(temp_list[2])\n",
        "    w_cl = k_cl / (n_cl_in + n_cl_ex)\n",
        "    w_cl_list.append([node, w_cl])\n",
        "    \n",
        "  w_cl_df = pd.DataFrame(w_cl_list, columns=['node', 'w_cl'])\n",
        "  label_node = list(w_cl_df[w_cl_df.w_cl == w_cl_df.w_cl.max()].iloc[0])\n",
        "  return label_node[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "syJjQRd_IWFg"
      },
      "source": [
        "def createGraph(clusters, node_pair_with_sigma, time_label, country):\n",
        "  graph = []\n",
        "  for cluster in clusters:\n",
        "    edge_internal = \"\"\n",
        "    edge_external = \"\"\n",
        "    node_external = \"\"\n",
        "    node_internal = \"\"\n",
        "    density_sum = 0\n",
        "    internal_link_count = 0\n",
        "    centrality_sum = 0\n",
        "\n",
        "    node_external_list = []\n",
        "    for index, row in node_pair_with_sigma.iterrows():\n",
        "      if row[\"node_a\"] in cluster and row[\"node_b\"] in cluster:\n",
        "        edge_internal = edge_internal + row[\"node_a\"] + '#' + row[\"node_b\"] + '#' + str(row[\"sigma\"]) + '\"'\n",
        "        density_sum = density_sum + float(\"{:.2f}\".format(row[\"sigma\"]))\n",
        "        internal_link_count = internal_link_count + 1\n",
        "      elif row[\"node_a\"] in cluster or row[\"node_b\"] in cluster:\n",
        "        edge_external = edge_external + row[\"node_a\"] + '#' + row[\"node_b\"] + '#' + str(row[\"sigma\"]) + '\"'\n",
        "        centrality_sum = centrality_sum + float(\"{:.2f}\".format(np.square(row[\"sigma\"])))\n",
        "        if row[\"node_a\"] not in cluster:\n",
        "          node_external_list.append(row[\"node_a\"])\n",
        "        elif row[\"node_b\"] not in cluster:\n",
        "          node_external_list.append(row[\"node_b\"])\n",
        "    node_external_list = list(set(node_external_list))\n",
        "    for item in node_external_list:\n",
        "      node_external = node_external + item + \"#\"\n",
        "    for item in cluster:\n",
        "      node_internal = node_internal + item + \"#\"\n",
        "    # calculation of Density and Centrailty\n",
        "    density = density_sum / internal_link_count\n",
        "    centrality = np.sqrt(centrality_sum)\n",
        "    label_node = getLabelNode(edge_internal, edge_external, node_internal, node_external)\n",
        "\n",
        "    graph.append([label_node, density, centrality, time_label, country, node_internal, edge_internal, edge_external, node_external])\n",
        "    \n",
        "  graph_df = pd.DataFrame(graph, columns=['Label', 'Density', 'Centrality', 'Time_Label', 'Country', 'Node_Internal', 'Edge_Internal', 'Edge_External', 'Node_External'])\n",
        "  \n",
        "  # add Quadrant to it\n",
        "  Centrailty_median = graph_df[\"Centrality\"].median()\n",
        "  density_median = graph_df[\"Density\"].median()\n",
        "  \n",
        "  final_graph_list = []\n",
        "\n",
        "  for index, row in graph_df.iterrows():\n",
        "    if row[\"Density\"] >= density_median and row[\"Centrality\"] >= Centrailty_median:\n",
        "      quadrant = 1\n",
        "    elif row[\"Density\"] <= density_median and row[\"Centrality\"] >= Centrailty_median:\n",
        "      quadrant = 2\n",
        "    elif row[\"Density\"] >= density_median and row[\"Centrality\"] <= Centrailty_median:\n",
        "      quadrant = 3\n",
        "    elif row[\"Density\"] <= density_median and row[\"Centrality\"] <= Centrailty_median:\n",
        "      quadrant = 4\n",
        "    else:\n",
        "      quadrant = 1\n",
        "    \n",
        "    final_graph_list.append([row[\"Label\"], row[\"Density\"], row[\"Centrality\"], quadrant, row[\"Time_Label\"], row[\"Country\"], row[\"Node_Internal\"], row[\"Edge_Internal\"], row[\"Edge_External\"], row[\"Node_External\"]])\n",
        "  final_graph_df = pd.DataFrame(final_graph_list, columns=['Label', 'Density', 'Centrality', 'Quadrant', 'Time_Label', 'Country', 'Node_Internal', 'Edge_Internal', 'Edge_External', 'Node_External'])\n",
        "  return final_graph_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9T9_5PfvJmSD"
      },
      "source": [
        "# graph_t1 = createGraph(clusters_t1, node_pair_with_sigma_t1, \"JanMar2017\" ,\"de\")\n",
        "# graph_t1.to_csv(\"/content/drive/My Drive/Colab Notebooks/Particle Density Algo/results/JanMar2017.csv\")\n",
        "# graph_t1.head(10)\n",
        "\n",
        "# graph_t2 = createGraph(clusters_t2, node_pair_with_sigma_t2, \"AprJun2017\" ,\"de\")\n",
        "# graph_t2.to_csv(\"/content/drive/My Drive/Colab Notebooks/Particle Density Algo/results/AprJun2017.csv\")\n",
        "# graph_t2.head(10)\n",
        "\n",
        "# graph_t3 = createGraph(clusters_t3, node_pair_with_sigma_t3, \"JulSep2017\" ,\"de\")\n",
        "# graph_t3.to_csv(\"/content/drive/My Drive/Colab Notebooks/Particle Density Algo/results/JulSep2017.csv\")\n",
        "# graph_t3.head(10)\n",
        "\n",
        "# graph_t4 = createGraph(clusters_t4, node_pair_with_sigma_t4, \"OctDec2017\" ,\"de\")\n",
        "# graph_t4.to_csv(\"/content/drive/My Drive/Colab Notebooks/Particle Density Algo/results/OctDec2017.csv\")\n",
        "# graph_t4.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBCQYA5nLY1y"
      },
      "source": [
        "countries = ['gb', 'nl', 'fr', 'de']\n",
        "time_frames_files = [\n",
        "                      ['/T1/edgelist-JanMar2016.csv', '/T1/node-JanMar2016.csv', 'JanMar2016'],\n",
        "                      ['/T2/edgelist-AprJun2016.csv', '/T2/node-AprJun2016.csv', 'AprJun2016'],\n",
        "                      ['/T3/edgelist-JulSep2016.csv', '/T3/node-JulSep2016.csv', 'JulSep2016'],\n",
        "                      ['/T4/edgelist-OktDes2016.csv', '/T4/node-OktDes2016.csv', 'OctDec2016'],\n",
        "                      ['/T5/edgelist-JanMar2017.csv', '/T5/node-JanMar2017.csv', 'JanMar2017'],\n",
        "                      ['/T6/edgelist-AprJun2017.csv', '/T6/node-AprJun2017.csv', 'AprJun2017'],\n",
        "                      ['/T7/edgelist-JulSep2017.csv', '/T7/node-JulSep2017.csv', 'JulSep2017'],\n",
        "                      ['/T8/edgelist-OktDes2017-10countries.csv', '/T8/node-OktDes2017-10countries.csv', 'OctDec2017']\n",
        "                    ]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OMbTGIO7RbKx"
      },
      "source": [
        "source_path = '/content/drive/My Drive/Colab Notebooks/Particle Density Algo/raw_data/Data' \n",
        "result_path = '/content/drive/My Drive/Colab Notebooks/Particle Density Algo/results/'\n",
        "for country in countries:\n",
        "  for tf in time_frames_files:\n",
        "    print(tf[2] + \"_\" + country)\n",
        "    co_occurence_path = source_path + tf[0]\n",
        "    occurence_path = source_path + tf[1]\n",
        "    \n",
        "    time_frame = tf[2]\n",
        "    result_file_path = result_path + time_frame + \"_\" + country + \".csv\"\n",
        "    print(result_file_path)\n",
        "\n",
        "    co_occ_df, occ_df, df_neighbours, unique_nodes_pairs, node_pair_with_sigma = dataPreparation(co_occurence_path, occurence_path, country)\n",
        "\n",
        "    mean_sigma = node_pair_with_sigma[\"sigma\"].mean()\n",
        "    mean_sigma = float(\"{:.2f}\".format(mean_sigma))\n",
        "    \n",
        "    optimal_eps, clusters, noise_nodes, best_modularity, eps_series, mod_series = findBestEps(mean_sigma, df_neighbours, node_pair_with_sigma)\n",
        "\n",
        "    graph = createGraph(clusters, node_pair_with_sigma, time_frame , country)\n",
        "    print(\"eps \", eps_series)\n",
        "    print(\"mod \", mod_series)\n",
        "    print(\"=====================\")\n",
        "    graph.to_csv(result_file_path, index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Db3bvi57YM3a"
      },
      "source": [
        "best_modularity"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FYSBQ9hVQ_y-"
      },
      "source": [
        "co_occurence_path_t1 = '/content/drive/My Drive/Colab Notebooks/Particle Density Algo/raw_data/Data/T8/edgelist-OktDes2017-10countries.csv'\n",
        "occurence_path_t1 = '/content/drive/My Drive/Colab Notebooks/Particle Density Algo/raw_data/Data/T8/node-OktDes2017-10countries.csv'\n",
        "\n",
        "co_occ_df_t1, occ_df_t1, df_neighbours_t1, unique_nodes_pairs_t1, node_pair_with_sigma_t1 = dataPreparation(co_occurence_path_t1, occurence_path_t1, 'de')\n",
        "\n",
        "mean_sigma = node_pair_with_sigma_t1[\"sigma\"].mean()\n",
        "mean_sigma = float(\"{:.2f}\".format(mean_sigma))\n",
        "print(\"Mean Sigma \", mean_sigma)\n",
        "optimal_eps_t4, clusters_t4, noise_nodes_t4, modularity_t4 = findBestEps(mean_sigma, min_points, df_neighbours_t1, node_pair_with_sigma_t1)\n",
        "\n",
        "\n",
        "# clusters_t1, noise_nodes_t1 = ClusteringProcess(0.04, min_points, df_neighbours_t1, node_pair_with_sigma_t1)\n",
        "# Q_mid_t1 = getModularity(node_pair_with_sigma_t1, clusters_t1)\n",
        "\n",
        "# graph_t1 = createGraph(clusters_t1, node_pair_with_sigma_t1, \"OktDes2016\" ,\"de\")\n",
        "# graph_t1.to_csv(\"/content/drive/My Drive/Colab Notebooks/Particle Density Algo/results/OktDes2016.csv\", index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C3EzuigUNYKH"
      },
      "source": [
        "modularity_t4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PqiPTUJzlYC4"
      },
      "source": [
        "# devops#microservices - 2 co-occ\n",
        "occ_df_t1.query(\"skill == 'microservices'\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RqVY18dNRh-h"
      },
      "source": [
        "graph_t1 = createGraph(clusters_t1, node_pair_with_sigma_t1, \"OktDes2016\" ,\"de\")\n",
        "graph_t1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SCsmKkYsQ9my"
      },
      "source": [
        "# Prepare Json from Graph CSV"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eo9aXJtrHNF7"
      },
      "source": [
        "# Prepare Json file\n",
        "def prepareGraphJson(graph_df, filename):\n",
        "  filename = '/content/drive/My Drive/Colab Notebooks/Particle Density Algo/results/json/'+ filename + '.json'\n",
        "  # create a json with nodes and edges tags\n",
        "  with open(filename, 'w') as fp: \n",
        "      fp.write('{\"nodes\":[], \"edges\":[]}')\n",
        "  # Append json tags\n",
        "  with open(filename) as json_file: \n",
        "    data = json.load(json_file) \n",
        "      \n",
        "    node_obj = data['nodes'] \n",
        "    edge_obj = data['edges']\n",
        "\n",
        "    cluster_index = 1\n",
        "    all_cluster_node_list = []\n",
        "    all_external_node_list = []\n",
        "    for index, row in graph_df.iterrows():\n",
        "      # Writing Nodes to file\n",
        "      cluster_nodes = row[\"Node_Internal\"].split('#')\n",
        "      for n in cluster_nodes:\n",
        "        if n:\n",
        "          node = {\n",
        "              \"id\": n,\n",
        "              \"group\": cluster_index\n",
        "          }\n",
        "          node_obj.append(node)\n",
        "          all_cluster_node_list.append(n)\n",
        "      cluster_index = cluster_index + 1\n",
        "\n",
        "      # Process Edge Internal\n",
        "      internal_edges = row[\"Edge_Internal\"].split('\"')\n",
        "      for e in internal_edges:\n",
        "        if e:\n",
        "          e_arr = e.split('#')\n",
        "          edge = {\n",
        "              \"from\": e_arr[0],\n",
        "              \"to\": e_arr[1]\n",
        "          }\n",
        "          edge_obj.append(edge)\n",
        "      # Process Edge Extrnal\n",
        "      external_edges = row[\"Edge_External\"].split('\"')\n",
        "      for e in external_edges:\n",
        "        if e:\n",
        "          e_arr = e.split('#')\n",
        "          edge = {\n",
        "              \"from\": e_arr[0],\n",
        "              \"to\": e_arr[1]\n",
        "          }\n",
        "          edge_obj.append(edge)\n",
        "          all_external_node_list.append(e_arr[0])\n",
        "          all_external_node_list.append(e_arr[1])\n",
        "\n",
        "    all_external_node_list = list(set(all_external_node_list))\n",
        "    for n in all_external_node_list:\n",
        "      if n not in all_cluster_node_list:\n",
        "        node = {\n",
        "            \"id\": n,\n",
        "            \"group\": 0\n",
        "        }\n",
        "        node_obj.append(node)\n",
        "\n",
        "  # writing data to json\n",
        "  with open(filename,'w') as f: \n",
        "    json.dump(data, f, indent=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cpKCSZ4LUol-"
      },
      "source": [
        "jan_mar_2017_graph = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/Particle Density Algo/results/JanMar2017.csv\")\n",
        "prepareGraphJson(jan_mar_2017_graph, 'jan_mar2017')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EdEgCctt_mm8"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}